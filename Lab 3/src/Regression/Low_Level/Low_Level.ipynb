{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import essential libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from geopy.distance import geodesic\n",
    "from shapely.geometry import Point\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Spark Context variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/09 16:39:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low_Level_Regression\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(\"local[*]\", \"Low_Level_Regression\")\n",
    "print(sc.appName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the maximum number of workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the GeoJSON file of NYC\n",
    "nyc = gpd.read_file(\"new-york-city-boroughs.geojson\")\n",
    "\n",
    "# Convert NYC regions into a list of Polygons\n",
    "nyc_polygons = nyc['geometry'].tolist()\n",
    "broadcast_nyc = sc.broadcast(nyc_polygons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['id,vendor_id,pickup_datetime,dropoff_datetime,passenger_count,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,store_and_fwd_flag,trip_duration',\n",
       " 'id2875421,2,2016-03-14 17:24:55,2016-03-14 17:32:30,1,-73.982154846191406,40.767936706542969,-73.964630126953125,40.765602111816406,N,455',\n",
       " 'id2377394,1,2016-06-12 00:43:35,2016-06-12 00:54:38,1,-73.980415344238281,40.738563537597656,-73.999481201171875,40.731151580810547,N,663',\n",
       " 'id3858529,2,2016-01-19 11:35:24,2016-01-19 12:10:48,1,-73.979026794433594,40.763938903808594,-74.005332946777344,40.710086822509766,N,2124',\n",
       " 'id3504673,2,2016-04-06 19:32:31,2016-04-06 19:39:40,1,-74.010040283203125,40.719970703125,-74.01226806640625,40.706718444824219,N,429']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV file as a text file and filter out the header\n",
    "lines = sc.textFile(\"train.csv\")\n",
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Parse the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the lines from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "header = lines.first().split(',')\n",
    "data = lines.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Pre-processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Select columns from the parsed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `store_and_fwd_flag` feature indicates whether the trip record was stored in the vehicle's memory before being sent to the vendor, due to the vehicle not having a connection to the server. 'Y' means store and forward, while 'N' means the trip was not a store and forward trip. Since this feature does not provide significant value in predicting trip duration, we have chosen to remove it from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecte features to convert to float\n",
    "float_cols = [\n",
    "'vendor_id',\n",
    " 'passenger_count',\n",
    " 'pickup_longitude',\n",
    " 'pickup_latitude',\n",
    " 'dropoff_longitude',\n",
    " 'dropoff_latitude',\n",
    " 'trip_duration'\n",
    "]\n",
    "\n",
    "# Get the selected features' indices in the shown above header\n",
    "float_indices = [header.index(col) for col in float_cols]\n",
    "broadcast_indices = sc.broadcast(float_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the data and add more features to detect outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(line):\n",
    "    float_indices = broadcast_indices.value\n",
    "    try:\n",
    "        parts = line.strip().split(',')\n",
    "        \n",
    "        # TIME FEATURES\n",
    "        dt_start_str = parts[2]  # pick up time\n",
    "        dt_end_str = parts[3]  # drop off time\n",
    "        dt_start = datetime.strptime(dt_start_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "        dt_end = datetime.strptime(dt_end_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "        trip_duration = (dt_end - dt_start).total_seconds() # Add trip duration feature for later consistency checking\n",
    "        \n",
    "        # Extract feature: hour, weekday, day of year\n",
    "        time_features = [dt_start.hour, dt_start.weekday(), dt_start.timetuple().tm_yday]\n",
    "\n",
    "        # Parse numerical values\n",
    "        parsed = [float(parts[i]) for i in float_indices]\n",
    "        \n",
    "        return tuple(parsed + time_features + [trip_duration])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing line: {line} -> {e}\")\n",
    "        return None  # Print log's information and return None when encounter error in processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove inconsisent rows and duplicated rows in dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedData = data.map(parse_data).filter(lambda x: x is not None)\n",
    "                        \n",
    "# Remove rows that have trip duration not equal to \n",
    "# result calculated from pick up date time and drop off date time\n",
    "rdd_data = parsedData.filter(lambda x: int(x[6]) == int(x[10]))\n",
    "\n",
    "rdd_data = rdd_data.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_data.take(1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there is no error log on the cell above (which performs converting datatype from string to float), we assume that the dataset does not have `NaN` value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Remove rows with invalid pick up/drop off location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_nyc(lon, lat):\n",
    "    pt = Point(lon, lat)\n",
    "    for polygon in broadcast_nyc.value:\n",
    "        if polygon.contains(pt):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def check_row(row):\n",
    "    \"\"\"Remove rows that have neither pick up location nor drop off location in New York City\"\"\"\n",
    "    pickup_in = is_in_nyc(row[2], row[3])\n",
    "    dropoff_in = is_in_nyc(row[4], row[5])\n",
    "    return pickup_in or dropoff_in  # hoặc tùy chọn: pickup_in or dropoff_in\n",
    "\n",
    "rdd_data = rdd_data.filter(check_row).persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset to train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rdd, test_rdd = rdd_data.randomSplit([0.7,0.3], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will add some more features to train set to detect outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_data(line):\n",
    "    try:\n",
    "        # SPATIAL FEATURE\n",
    "        start_location = (line[3], line[2])\n",
    "        end_location = (line[5], line[4])\n",
    "        direct_distance = geodesic(start_location, end_location).meters # Add direct distance feature for later logic checking\n",
    "        \n",
    "        # VELOCITY FEATURE \n",
    "        velocity = direct_distance / line[-1] * 3.6 # Add velocity feature for later logic checking (km/h)\n",
    "        return line[:6] + line[7:] + (direct_distance, velocity)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing line: {line} -> {e}\")\n",
    "        return None  # Print log's information and return None when encounter error in processing\n",
    "    \n",
    "train_rdd = train_rdd.map(add_features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rdd = train_rdd.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features of the training set are listed below in order:\n",
    "- vendor id\n",
    "- passenger count\n",
    "- pickup longitude\n",
    "- pickup latitude\n",
    "- dropoff longitude\n",
    "- dropoff latitude\n",
    "- hour\n",
    "- weekday\n",
    "- yearday\n",
    "- trip duration\n",
    "- direct distance\n",
    "- velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Remove outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the referenced EDA, the columns we need to preprocess are all non-normally distributed. Therefore, we need to use appropriate methods to detect and remove as many outliers as possible.\n",
    "\n",
    "Additionally, we need to define a function for statistical reporting to ensure that these methods are on the right track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm thu thập giá trị cột\n",
    "def collect_column_values(row):\n",
    "    num_columns = num_columns_broadcast.value\n",
    "    offset = offset_broadcast.value\n",
    "    return [(col_idx, [row[col_idx]]) for col_idx in range(num_columns - offset, num_columns)]\n",
    "\n",
    "def statistical_report(values):\n",
    "    values = np.array(values, dtype=float)\n",
    "    return (values.min(), values.max(), values.mean(), np.median(values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collect values at each selected column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/09 16:41:39 WARN BlockManager: Task 15 already completed, not releasing lock for rdd_10_0\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Because we want to collect the last 3 columns, we create \n",
    "# a broadcast variable to pass to collect_column_values function\n",
    "offset_broadcast = sc.broadcast(3)\n",
    "# Số cột\n",
    "num_columns_broadcast = sc.broadcast(len(train_rdd.take(1)[0]))\n",
    "\n",
    "# Thu thập giá trị cột\n",
    "col_values_rdd = train_rdd.flatMap(collect_column_values)\n",
    "col_values_grouped = col_values_rdd.reduceByKey(lambda x, y: x + y).persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we come with IQR approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def calculate_bounds(values):\n",
    "    values = np.array(values, dtype=float)\n",
    "    Q1 = np.percentile(values, 25)\n",
    "    Q3 = np.percentile(values, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    return (Q1 - 1.5 * IQR, Q3 + 1.5 * IQR)\n",
    "\n",
    "# Tính bounds song song\n",
    "bounds_rdd = col_values_grouped.mapValues(calculate_bounds)\n",
    "bounds = bounds_rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating lower bound and upper bound of each column, let's take a look at them (to check whether the bounds are reasonable or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 9: Lower bound = -618.5, Upper bound = 2089.5\n",
      "Column 10: Lower bound = -2725.0618484028378, Upper bound = 7828.676667884347\n",
      "Column 11: Lower bound = -3.9613838811481177, Upper bound = 30.93929904258277\n"
     ]
    }
   ],
   "source": [
    "# In bounds\n",
    "for col_idx, (lower, upper) in bounds.items():\n",
    "    print(f\"Column {col_idx}: Lower bound = {lower}, Upper bound = {upper}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns 9, 10, and 11 sequentially represent `trip duration`, `direct distance`, and `velocity`. To be honest, a `trip duration` of more than 2000 seconds (~33 minutes) cannot be considered an outlier. The lower bound is a negative float, so there is nothing to analyze in that regard. I'm not entirely sure how the infrastructure in New York City is distributed, but a `direct distance` greater than 7.8 km can be considered a tolerable upper bound. The upper bound for `velocity` might be reasonable because, given the traffic conditions in NYC, an average speed of 31 km/h (which would likely be higher in reality) is not achievable. Therefore, we will try applying the IQR method for both `velocity` and `direct distance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lọc outliers\n",
    "def IQR_outlier(row):\n",
    "    num_columns = num_columns_broadcast.value\n",
    "    offset = offset_broadcast.value\n",
    "    for col_idx in range(num_columns - offset, num_columns):\n",
    "        value = row[col_idx]\n",
    "        lower_bound, upper_bound = bounds[col_idx]\n",
    "        if value < lower_bound or value > upper_bound:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "offset_broadcast = sc.broadcast(2)\n",
    "filtered_iqr_rdd = train_rdd.filter(IQR_outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's print the statistical report after apply IQR for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Direct Distance-----\n",
      "Min: 0.0\n",
      "Max: 7828.372302664765\n",
      "Mean: 2356.09453026498\n",
      "Median: 1889.4171528723853\n",
      "\n",
      "-----Velocity-----\n",
      "Min: 0.0\n",
      "Max: 30.939142108169577\n",
      "Mean: 12.776125067547367\n",
      "Median: 11.996498056419297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "name_col = [\"Direct Distance\", \"Velocity\"]\n",
    "# Because we want to collect the last 2 columns so we create \n",
    "# a broadcast variable to pass to collect_column_values function\n",
    "offset_broadcast = sc.broadcast(2)\n",
    "\n",
    "# Thu thập giá trị cột\n",
    "col_iqr_rdd = filtered_iqr_rdd.flatMap(collect_column_values)\n",
    "col_iqr_grouped = col_iqr_rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Calculate statistical report for each column \n",
    "iqr_filtered_rdd = col_iqr_grouped.mapValues(statistical_report)\n",
    "iqr_report = iqr_filtered_rdd.collectAsMap()\n",
    "\n",
    "# Print the statistical report for IQR filtered column\n",
    "for i in range(2):\n",
    "    print(f\"\\n-----{name_col[i]}-----\")\n",
    "    report = iqr_report[num_columns_broadcast.value - 2 + i]\n",
    "    print(f\"Min: {report[0]}\")\n",
    "    print(f\"Max: {report[1]}\")\n",
    "    print(f\"Mean: {report[2]}\")\n",
    "    print(f\"Median: {report[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower bound is not good enough to handle those outliers. Maybe we should try another approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we move to robust Z-score approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Hàm thu thập giá trị cột\n",
    "def collect_column_values_abs_med(row):\n",
    "    num_columns = num_columns_broadcast.value\n",
    "    meds = meds_broadcast.value\n",
    "    return [(col_idx, [np.abs(row[col_idx] - meds[col_idx])]) for col_idx in range(num_columns - 3, num_columns)]\n",
    "\n",
    "def calculate_med(values):\n",
    "    values = np.array(values, dtype=float)\n",
    "    return np.median(values)\n",
    "\n",
    "# Tính median từng cột song song\n",
    "meds_rdd = col_values_grouped.mapValues(calculate_med)\n",
    "meds = meds_rdd.collectAsMap()\n",
    "\n",
    "meds_broadcast = sc.broadcast(meds)\n",
    "\n",
    "# Thu thập giá trị cột\n",
    "col_values_abs_med_rdd = train_rdd.flatMap(collect_column_values_abs_med)\n",
    "col_values_abs_med_grouped = col_values_abs_med_rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Tính mad song song\n",
    "mads_rdd = col_values_abs_med_grouped.mapValues(calculate_med)\n",
    "mads = mads_rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[16] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_values_grouped.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the outliers in selected columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lọc outliers\n",
    "def robust_column_rdd(row):\n",
    "    num_columns = num_columns_broadcast.value\n",
    "    meds = meds_broadcast.value\n",
    "    mads = mads_broadcast.value\n",
    "    return [(col_idx, np.abs(row[col_idx] - meds[col_idx]) / mads[col_idx]) \n",
    "            for col_idx in range(num_columns - 3, num_columns)]\n",
    "    \n",
    "robust_rdd = train_rdd.flatMap(robust_column_rdd).reduceByKey(lambda x,y: x + y)\n",
    "\n",
    "# def calculate_robust_bounds(values):\n",
    "#     values = np.array(values, dtype=float)\n",
    "#     return np.percentile(values, 99.5)\n",
    "\n",
    "# threshold_rdd = robust_rdd.mapValues(calculate_robust_bounds)\n",
    "# threshold = threshold_rdd.collectAsMap()\n",
    "\n",
    "# Lọc outliers\n",
    "def robust_outlier(row):\n",
    "    num_columns = num_columns_broadcast.value\n",
    "    meds = meds_broadcast.value\n",
    "    mads = mads_broadcast.value\n",
    "    # threshold = threshold_broadcast.value\n",
    "    for col_idx in range(num_columns - 3, num_columns):\n",
    "        if np.abs(row[col_idx] - meds[col_idx]) / mads[col_idx] > 3:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "mads_broadcast = sc.broadcast(mads)\n",
    "# threshold_broadcast = sc.broadcast(threshold)\n",
    "filtered_robust_rdd = train_rdd.filter(robust_outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's print the statistical report after applying robust Z-score for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Trip duration-----\n",
      "Min: 1.0\n",
      "Max: 1595.0\n",
      "Mean: 624.868964779068\n",
      "Median: 569.0\n",
      "\n",
      "-----Direct Distance-----\n",
      "Min: 0.4236109090686456\n",
      "Max: 5277.969420283912\n",
      "Mean: 1981.7221711158975\n",
      "Median: 1723.4443160613012\n",
      "\n",
      "-----Velocity-----\n",
      "Min: 0.2577348925578789\n",
      "Max: 25.343768423001187\n",
      "Mean: 12.20203328569908\n",
      "Median: 11.678126558060292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "name_col = [\"Trip duration\",\"Direct Distance\", \"Velocity\"]\n",
    "# Because we want to collect the last 3 columns so we create \n",
    "# a broadcast variable to pass to collect_column_values function\n",
    "offset_broadcast = sc.broadcast(3)\n",
    "\n",
    "# Thu thập giá trị cột\n",
    "col_robust_rdd = filtered_robust_rdd.flatMap(collect_column_values)\n",
    "col_robust_grouped = col_robust_rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Calculate statistical report for each column \n",
    "robust_filtered_rdd = col_robust_grouped.mapValues(statistical_report)\n",
    "robust_report = robust_filtered_rdd.collectAsMap()\n",
    "\n",
    "# Print the statistical report for IQR filtered column\n",
    "for i in range(3):\n",
    "    print(f\"\\n-----{name_col[i]}-----\")\n",
    "    report = robust_report[num_columns_broadcast.value - 3 + i]\n",
    "    print(f\"Min: {report[0]}\")\n",
    "    print(f\"Max: {report[1]}\")\n",
    "    print(f\"Mean: {report[2]}\")\n",
    "    print(f\"Median: {report[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the statistical report, looks like it even get worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_rdd = filtered_iqr_rdd.filter(lambda x: x[-1] > 0 and x[-1] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/09 17:10:56 WARN BlockManager: Task 70 already completed, not releasing lock for rdd_38_0\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([  2.        ,   1.        , -74.01004028,  40.7199707 ,\n",
      "       -74.01226807,  40.70671844,  19.        ,   2.        ,\n",
      "        97.        ]), 429.0)\n",
      "<class 'tuple'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'float'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:=========>                                                (1 + 5) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "889018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create an RDD where each record is a tuple: (features as a numpy array, label)\n",
    "cleaned_data = cleaned_rdd.map(lambda cols: (np.array(cols[:-3]), cols[-3])).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "first_element = cleaned_data.take(1)[0]\n",
    "print(first_element)\n",
    "print(type(first_element)) \n",
    "print(type(first_element[0]))\n",
    "print(type(first_element[1]))\n",
    "print(cleaned_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[10] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleaned_data consist of lazy evaluation operation on train_rdd, so we have to make sure that \n",
    "# train_rdd is not unpersisted before cleaned_data call a non lazy evaluation operation and being cached\n",
    "train_rdd.unpersist() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the hyperparameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "889018"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lớp Node\n",
    "class Node:\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tính số lượng phân vị tăng theo kích thước dataset\n",
    "def get_adaptive_quantiles(rdd, feature_index, mode='sqrt'):\n",
    "    count = rdd.count()\n",
    "\n",
    "    if mode == 'sqrt':\n",
    "        num_quantiles = int(math.sqrt(count))\n",
    "    elif mode == 'log':\n",
    "        num_quantiles = int(math.log2(count))\n",
    "    else:\n",
    "        num_quantiles = 10  # fallback\n",
    "\n",
    "    values = rdd.map(lambda x: x[0][feature_index]) \\\n",
    "                .distinct() \\\n",
    "                .sortBy(lambda x: x) \\\n",
    "                .collect()\n",
    "\n",
    "    if len(values) <= num_quantiles:\n",
    "        return values\n",
    "\n",
    "    step = len(values) // (num_quantiles + 1)\n",
    "    quantiles = [values[i * step] for i in range(1, num_quantiles + 1)]\n",
    "    return quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm tìm phân chia tối ưu trên RDD\n",
    "def find_best_split_rdd(rdd, num_features):\n",
    "    def evaluate_split(feature_index):\n",
    "        quantiles = get_adaptive_quantiles(rdd, feature_index)\n",
    "\n",
    "        best_mse = float('inf')\n",
    "        best_threshold = None\n",
    "\n",
    "        for threshold in quantiles:\n",
    "            # Hàm seq_op trả về 6 giá trị: sumL, sq_sumL, countL, sumR, sq_sumR, countR\n",
    "            def seq_op(acc, row):\n",
    "                value = row[0][feature_index]\n",
    "                label = row[1]\n",
    "                if value <= threshold:\n",
    "                    return (\n",
    "                        acc[0] + label,           # sumL\n",
    "                        acc[1] + label ** 2,      # sq_sumL\n",
    "                        acc[2] + 1,               # countL\n",
    "                        acc[3], acc[4], acc[5]    # right giữ nguyên\n",
    "                    )\n",
    "                else:\n",
    "                    return (\n",
    "                        acc[0], acc[1], acc[2],   # left giữ nguyên\n",
    "                        acc[3] + label,           # sumR\n",
    "                        acc[4] + label ** 2,      # sq_sumR\n",
    "                        acc[5] + 1                # countR\n",
    "                    )\n",
    "\n",
    "            def comb_op(acc1, acc2):\n",
    "                return tuple(a + b for a, b in zip(acc1, acc2))\n",
    "\n",
    "            # Khởi tạo 6 giá trị ban đầu\n",
    "            init = (0.0, 0.0, 0, 0.0, 0.0, 0)\n",
    "            sumL, sq_sumL, countL, sumR, sq_sumR, countR = rdd.aggregate(init, seq_op, comb_op)\n",
    "\n",
    "            if countL == 0 or countR == 0:\n",
    "                continue\n",
    "\n",
    "            mse_left = (sq_sumL / countL) - (sumL / countL) ** 2\n",
    "            mse_right = (sq_sumR / countR) - (sumR / countR) ** 2\n",
    "            total_mse = (countL * mse_left + countR * mse_right) / (countL + countR)\n",
    "\n",
    "            if total_mse < best_mse:\n",
    "                best_mse = total_mse\n",
    "                best_threshold = threshold\n",
    "\n",
    "        return (feature_index, best_threshold, best_mse)\n",
    "\n",
    "\n",
    "    results = [evaluate_split(i) for i in range(num_features)]\n",
    "\n",
    "    # Chọn phân chia tốt nhất\n",
    "    best_result = min(results, key=lambda x: x[2])\n",
    "    return best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Hàm xây dựng cây\n",
    "def build_tree_rdd(rdd, num_features, depth=0, max_depth=3, min_samples=10):\n",
    "    rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    count = rdd.count()\n",
    "    \n",
    "    # Điều kiện dừng\n",
    "    if count < min_samples or depth >= max_depth:\n",
    "        mean = rdd.map(lambda x: x[1]).reduce(lambda x, y: x + y) / count\n",
    "        return Node(value=mean)\n",
    "    \n",
    "    # Tìm phân chia tối ưu\n",
    "    feature_index, threshold, mse = find_best_split_rdd(rdd, num_features)\n",
    "    \n",
    "    if feature_index is None or threshold is None:\n",
    "        mean = rdd.map(lambda x: x[1]).reduce(lambda x, y: x + y) / count\n",
    "        return Node(value=mean)\n",
    "    \n",
    "    # Phân chia RDD\n",
    "    left_rdd = rdd.filter(lambda x: x[0][feature_index] <= threshold)\n",
    "    right_rdd = rdd.filter(lambda x: x[0][feature_index] > threshold)\n",
    "    \n",
    "    # Đệ quy xây dựng nhánh\n",
    "    left_node = build_tree_rdd(left_rdd, num_features, depth + 1, max_depth, min_samples)\n",
    "    right_node = build_tree_rdd(right_rdd, num_features, depth + 1, max_depth, min_samples)\n",
    "    \n",
    "    rdd.unpersist()\n",
    "    \n",
    "    return Node(feature_index=feature_index, threshold=threshold, \n",
    "                left=left_node, right=right_node)\n",
    "\n",
    "\n",
    "# Xây dựng cây\n",
    "num_features = 9  # Số đặc trưng\n",
    "min_samples = cleaned_data.count() // 100\n",
    "tree = build_tree_rdd(cleaned_data, num_features, 0, 3, min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/10 03:03:44 WARN BlockManager: Task 501403 already completed, not releasing lock for rdd_88918_0\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([  1.        ,   1.        , -74.00899506,  40.71258926,\n",
      "       -73.99863434,  40.72280884,  12.        ,   5.        ,\n",
      "         9.        ]), 707.0)\n",
      "<class 'tuple'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'float'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 167144:=============================================>        (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create an RDD where each record is a tuple: (features as a numpy array, label)\n",
    "test_data = test_rdd.map(lambda cols: (np.array(cols[:6] + cols[7:-1]), cols[-1]))\n",
    "\n",
    "first_element = test_data.take(1)[0]\n",
    "print(first_element)\n",
    "print(type(first_element)) \n",
    "print(type(first_element[0]))\n",
    "print(type(first_element[1]))\n",
    "test_size =test_data.count()\n",
    "print(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_data.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm dự đoán\n",
    "def predict(tree, sample):\n",
    "    if tree.value is not None:\n",
    "        return tree.value\n",
    "    if sample[tree.feature_index] <= tree.threshold:\n",
    "        return predict(tree.left, sample)\n",
    "    return predict(tree.right, sample)\n",
    "\n",
    "prediction_rdd = test_data.map(lambda x: (x[1], predict(tree, x[0]))).persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV. Evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4608.925435997215\n"
     ]
    }
   ],
   "source": [
    "# Tính RMSE từ prediction_rdd\n",
    "def compute_rmse(prediction_rdd):\n",
    "    sum_squared_error, count = prediction_rdd.aggregate(\n",
    "        (0.0, 0),\n",
    "        lambda acc, x: (acc[0] + (x[0] - x[1]) ** 2, acc[1] + 1),\n",
    "        lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
    "    )\n",
    "    mse = sum_squared_error / count\n",
    "    rmse = math.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "rmse = compute_rmse(prediction_rdd)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005965947874605471\n"
     ]
    }
   ],
   "source": [
    "def r2_seq_op(acc, row):\n",
    "    y = row[0]\n",
    "    y_hat = row[1]\n",
    "    return (\n",
    "        acc[0] + y,            # sum y để tính mean\n",
    "        acc[1] + 1,            # count\n",
    "        acc[2] + (y - y_hat) ** 2,     # ss_res\n",
    "        acc[3] + y ** 2                 # ss_tot phần cần để tính variance sau\n",
    "    )\n",
    "\n",
    "def r2_comb_op(a, b):\n",
    "    return tuple(x + y for x, y in zip(a, b))\n",
    "\n",
    "sum_y, count, ss_res, sum_y_sq = prediction_rdd.aggregate((0.0, 0.0, 0.0, 0.0), r2_seq_op, r2_comb_op)\n",
    "mean_y = sum_y / count\n",
    "ss_tot = sum_y_sq - (sum_y ** 2) / count\n",
    "\n",
    "r2 = 1 - ss_res / ss_tot\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.unpersist\n",
    "prediction_rdd.unpersist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
