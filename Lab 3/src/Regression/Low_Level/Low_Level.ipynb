{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Decision Tree: Low-Level Operations Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import essential libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from geopy.distance import geodesic\n",
    "from shapely.geometry import Point\n",
    "import math\n",
    "import os, shutil\n",
    "from hdfs import InsecureClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Spark Context variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/10 08:38:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low_Level_Regression\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(\"local[*]\", \"Low_Level_Regression\")\n",
    "print(sc.appName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the maximum number of workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the GeoJSON file of NYC\n",
    "nyc = gpd.read_file(\"new-york-city-boroughs.geojson\")\n",
    "\n",
    "# Convert NYC regions into a list of Polygons\n",
    "nyc_polygons = nyc['geometry'].tolist()\n",
    "broadcast_nyc = sc.broadcast(nyc_polygons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['id,vendor_id,pickup_datetime,dropoff_datetime,passenger_count,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,store_and_fwd_flag,trip_duration',\n",
       " 'id2875421,2,2016-03-14 17:24:55,2016-03-14 17:32:30,1,-73.982154846191406,40.767936706542969,-73.964630126953125,40.765602111816406,N,455',\n",
       " 'id2377394,1,2016-06-12 00:43:35,2016-06-12 00:54:38,1,-73.980415344238281,40.738563537597656,-73.999481201171875,40.731151580810547,N,663',\n",
       " 'id3858529,2,2016-01-19 11:35:24,2016-01-19 12:10:48,1,-73.979026794433594,40.763938903808594,-74.005332946777344,40.710086822509766,N,2124',\n",
       " 'id3504673,2,2016-04-06 19:32:31,2016-04-06 19:39:40,1,-74.010040283203125,40.719970703125,-74.01226806640625,40.706718444824219,N,429']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV file as a text file and filter out the header\n",
    "lines = sc.textFile(\"hdfs:///hcmus/22120210/Lab3/data/train.csv\")\n",
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Parse the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the lines from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "header = lines.first().split(',')\n",
    "data = lines.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Pre-processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Select columns from the parsed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `store_and_fwd_flag` feature indicates whether the trip record was stored in the vehicle's memory before being sent to the vendor, due to the vehicle not having a connection to the server. 'Y' means store and forward, while 'N' means the trip was not a store and forward trip. Since this feature does not provide significant value in predicting trip duration, we have chosen to remove it from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecte features to convert to float\n",
    "float_cols = [\n",
    "'vendor_id',\n",
    " 'passenger_count',\n",
    " 'pickup_longitude',\n",
    " 'pickup_latitude',\n",
    " 'dropoff_longitude',\n",
    " 'dropoff_latitude',\n",
    " 'trip_duration'\n",
    "]\n",
    "\n",
    "# Get the selected features' indices in the shown above header\n",
    "float_indices = [header.index(col) for col in float_cols]\n",
    "broadcast_indices = sc.broadcast(float_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the data and add more features to detect outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(line):\n",
    "    float_indices = broadcast_indices.value\n",
    "    try:\n",
    "        parts = line.strip().split(',')\n",
    "        \n",
    "        # TIME FEATURES\n",
    "        dt_start_str = parts[2]  # pick up time\n",
    "        dt_end_str = parts[3]  # drop off time\n",
    "        dt_start = datetime.strptime(dt_start_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "        dt_end = datetime.strptime(dt_end_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "        trip_duration = (dt_end - dt_start).total_seconds() # Add trip duration feature for later consistency checking\n",
    "        \n",
    "        # Extract feature: hour, weekday, day of year\n",
    "        time_features = [dt_start.hour, dt_start.weekday(), dt_start.timetuple().tm_yday]\n",
    "\n",
    "        # Parse numerical values\n",
    "        parsed = [float(parts[i]) for i in float_indices]\n",
    "        \n",
    "        return tuple(parsed + time_features + [trip_duration])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing line: {line} -> {e}\")\n",
    "        return None  # Print log's information and return None when encounter error in processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove inconsisent rows and duplicated rows in dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedData = data.map(parse_data).filter(lambda x: x is not None)\n",
    "                        \n",
    "# Remove rows that have trip duration not equal to \n",
    "# result calculated from pick up date time and drop off date time\n",
    "rdd_data = parsedData.filter(lambda x: int(x[6]) == int(x[10]))\n",
    "\n",
    "rdd_data = rdd_data.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.0,\n",
       " 1.0,\n",
       " -74.01004028320312,\n",
       " 40.719970703125,\n",
       " -74.01226806640625,\n",
       " 40.70671844482422,\n",
       " 429.0,\n",
       " 19,\n",
       " 2,\n",
       " 97,\n",
       " 429.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_data.take(1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there is no error log on the cell above (which performs converting datatype from string to float), we assume that the dataset does not have `NaN` value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Remove rows with invalid pick up/drop off location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_nyc(lon, lat):\n",
    "    pt = Point(lon, lat)\n",
    "    for polygon in broadcast_nyc.value:\n",
    "        if polygon.contains(pt):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def check_row(row):\n",
    "    \"\"\"Remove rows that have neither pick up location nor drop off location in New York City\"\"\"\n",
    "    pickup_in = is_in_nyc(row[2], row[3])\n",
    "    dropoff_in = is_in_nyc(row[4], row[5])\n",
    "    return pickup_in or dropoff_in\n",
    "\n",
    "rdd_data = rdd_data.filter(check_row).persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset to train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rdd, test_rdd = rdd_data.randomSplit([0.7,0.3], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will add some more features to train set to detect outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_data(line):\n",
    "    try:\n",
    "        # SPATIAL FEATURE\n",
    "        start_location = (line[3], line[2])\n",
    "        end_location = (line[5], line[4])\n",
    "        direct_distance = geodesic(start_location, end_location).meters # Add direct distance feature for later logic checking\n",
    "        \n",
    "        # VELOCITY FEATURE \n",
    "        velocity = direct_distance / line[-1] * 3.6 # Add velocity feature for later logic checking (km/h)\n",
    "        return line[:6] + line[7:] + (direct_distance, velocity)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing line: {line} -> {e}\")\n",
    "        return None  # Print log's information and return None when encounter error in processing\n",
    "    \n",
    "train_rdd = train_rdd.map(add_features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rdd = train_rdd.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features of the training set are listed below in order:\n",
    "- vendor id\n",
    "- passenger count\n",
    "- pickup longitude\n",
    "- pickup latitude\n",
    "- dropoff longitude\n",
    "- dropoff latitude\n",
    "- hour\n",
    "- weekday\n",
    "- yearday\n",
    "- trip duration\n",
    "- direct distance\n",
    "- velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Remove outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the referenced EDA, the columns we need to preprocess are all non-normally distributed. Therefore, we need to use appropriate methods to detect and remove as many outliers as possible.\n",
    "\n",
    "Additionally, we need to define a function for statistical reporting to ensure that these methods are on the right track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm thu thập giá trị cột\n",
    "def collect_column_values(row):\n",
    "    num_columns = num_columns_broadcast.value\n",
    "    offset = offset_broadcast.value\n",
    "    return [(col_idx, [row[col_idx]]) for col_idx in range(num_columns - offset, num_columns)]\n",
    "\n",
    "def statistical_report(values):\n",
    "    values = sorted([float(v) for v in values])\n",
    "    n = len(values)\n",
    "    \n",
    "    min_val = values[0]\n",
    "    max_val = values[-1]\n",
    "    mean_val = sum(values) / n\n",
    "\n",
    "    # Tính median\n",
    "    if n % 2 == 1:\n",
    "        median_val = values[n // 2]\n",
    "    else:\n",
    "        median_val = (values[n // 2 - 1] + values[n // 2]) / 2\n",
    "\n",
    "    return (min_val, max_val, mean_val, median_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collect values at each selected column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/10 10:19:10 WARN BlockManager: Task 15 already completed, not releasing lock for rdd_11_0\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Because we want to collect the last 3 columns, we create \n",
    "# a broadcast variable to pass to collect_column_values function\n",
    "offset_broadcast = sc.broadcast(3)\n",
    "# Number of columns\n",
    "num_columns_broadcast = sc.broadcast(len(train_rdd.take(1)[0]))\n",
    "\n",
    "# Collect values grouped by column\n",
    "col_values_rdd = train_rdd.flatMap(collect_column_values)\n",
    "col_values_grouped = col_values_rdd.reduceByKey(lambda x, y: x + y).persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we come with IQR approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def calculate_bounds(values):\n",
    "    # Convert to float and sort\n",
    "    values = sorted([float(v) for v in values])\n",
    "    n = len(values)\n",
    "    \n",
    "    def percentile(p):\n",
    "        k = (n - 1) * (p / 100)\n",
    "        f = int(k)\n",
    "        c = f + 1\n",
    "        if c >= n:\n",
    "            return values[f]\n",
    "        d0 = values[f] * (c - k)\n",
    "        d1 = values[c] * (k - f)\n",
    "        return d0 + d1\n",
    "\n",
    "    Q1 = percentile(25)\n",
    "    Q3 = percentile(75)\n",
    "    IQR = Q3 - Q1\n",
    "    return (Q1 - 1.5 * IQR, Q3 + 1.5 * IQR)\n",
    "\n",
    "# Calculate the bounds concurrently\n",
    "bounds_rdd = col_values_grouped.mapValues(calculate_bounds)\n",
    "bounds = bounds_rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating lower bound and upper bound of each column, let's take a look at them (to check whether the bounds are reasonable or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 9: Lower bound = -618.5, Upper bound = 2089.5\n",
      "Column 10: Lower bound = -2725.0618484028378, Upper bound = 7828.676667884347\n",
      "Column 11: Lower bound = -3.9613838811481177, Upper bound = 30.93929904258277\n"
     ]
    }
   ],
   "source": [
    "# Print all the bounds\n",
    "for col_idx, (lower, upper) in bounds.items():\n",
    "    print(f\"Column {col_idx}: Lower bound = {lower}, Upper bound = {upper}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns 9, 10, and 11 sequentially represent `trip duration`, `direct distance`, and `velocity`. To be honest, a `trip duration` of more than 2000 seconds (~33 minutes) cannot be considered an outlier. The lower bound is a negative float, so there is nothing to analyze in that regard. I'm not entirely sure how the infrastructure in New York City is distributed, but a `direct distance` greater than 7.8 km can be considered a tolerable upper bound. The upper bound for `velocity` might be reasonable because, given the traffic conditions in NYC, an average speed of 31 km/h (which would likely be higher in reality) is not achievable. Therefore, we will try applying the IQR method for both `velocity` and `direct distance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter all the outlies\n",
    "def IQR_outlier(row):\n",
    "    num_columns = num_columns_broadcast.value\n",
    "    offset = offset_broadcast.value\n",
    "    for col_idx in range(num_columns - offset, num_columns):\n",
    "        value = row[col_idx]\n",
    "        lower_bound, upper_bound = bounds[col_idx]\n",
    "        if value < lower_bound or value > upper_bound:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "offset_broadcast = sc.broadcast(2)\n",
    "filtered_iqr_rdd = train_rdd.filter(IQR_outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's print the statistical report after applying IQR for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Direct Distance-----\n",
      "Min: 0.0\n",
      "Max: 7828.372302664765\n",
      "Mean: 2356.09453026498\n",
      "Median: 1889.4171528723853\n",
      "\n",
      "-----Velocity-----\n",
      "Min: 0.0\n",
      "Max: 30.939142108169577\n",
      "Mean: 12.776125067547364\n",
      "Median: 11.996498056419297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "name_col = [\"Direct Distance\", \"Velocity\"]\n",
    "# Because we want to collect the last 2 columns so we create \n",
    "# a broadcast variable to pass to collect_column_values function\n",
    "offset_broadcast = sc.broadcast(2)\n",
    "\n",
    "# collect values grouped by columns\n",
    "col_iqr_rdd = filtered_iqr_rdd.flatMap(collect_column_values)\n",
    "col_iqr_grouped = col_iqr_rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Calculate statistical report for each column \n",
    "iqr_filtered_rdd = col_iqr_grouped.mapValues(statistical_report)\n",
    "iqr_report = iqr_filtered_rdd.collectAsMap()\n",
    "\n",
    "# Print the statistical report for IQR filtered column\n",
    "for i in range(2):\n",
    "    print(f\"\\n-----{name_col[i]}-----\")\n",
    "    report = iqr_report[num_columns_broadcast.value - 2 + i]\n",
    "    print(f\"Min: {report[0]}\")\n",
    "    print(f\"Max: {report[1]}\")\n",
    "    print(f\"Mean: {report[2]}\")\n",
    "    print(f\"Median: {report[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower bound is not good enough to handle those outliers. Maybe we should try another approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we move to robust Z-score approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def collect_column_values_abs_med(row):\n",
    "    num_columns = num_columns_broadcast.value\n",
    "    meds = meds_broadcast.value\n",
    "    return [(col_idx, [abs(row[col_idx] - meds[col_idx])]) for col_idx in range(num_columns - 3, num_columns)]\n",
    "\n",
    "def calculate_med(values):\n",
    "    values = sorted([float(v) for v in values])\n",
    "    n = len(values)\n",
    "    \n",
    "    def percentile(p):\n",
    "        k = (n - 1) * (p / 100)\n",
    "        f = int(k)\n",
    "        c = f + 1\n",
    "        if c >= n:\n",
    "            return values[f]\n",
    "        d0 = values[f] * (c - k)\n",
    "        d1 = values[c] * (k - f)\n",
    "        return d0 + d1\n",
    "    return percentile(50)\n",
    "\n",
    "# Calculate median of each column concurrently\n",
    "meds_rdd = col_values_grouped.mapValues(calculate_med)\n",
    "meds = meds_rdd.collectAsMap()\n",
    "\n",
    "meds_broadcast = sc.broadcast(meds)\n",
    "\n",
    "# Collect values grouped by columns\n",
    "col_values_abs_med_rdd = train_rdd.flatMap(collect_column_values_abs_med)\n",
    "col_values_abs_med_grouped = col_values_abs_med_rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Calculate MAD (Median Absolute Deviation) of each column concurrently\n",
    "mads_rdd = col_values_abs_med_grouped.mapValues(calculate_med)\n",
    "mads = mads_rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[17] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_values_grouped.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the outliers in selected columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the outliers\n",
    "def robust_column_rdd(row):\n",
    "    num_columns = num_columns_broadcast.value\n",
    "    meds = meds_broadcast.value\n",
    "    mads = mads_broadcast.value\n",
    "    return [(col_idx, abs(row[col_idx] - meds[col_idx]) / mads[col_idx]) \n",
    "            for col_idx in range(num_columns - 3, num_columns)]\n",
    "    \n",
    "robust_rdd = train_rdd.flatMap(robust_column_rdd).reduceByKey(lambda x,y: x + y)\n",
    "\n",
    "# Filter the outliers\n",
    "def robust_outlier(row):\n",
    "    num_columns = num_columns_broadcast.value\n",
    "    meds = meds_broadcast.value\n",
    "    mads = mads_broadcast.value\n",
    "    # threshold = threshold_broadcast.value\n",
    "    for col_idx in range(num_columns - 3, num_columns):\n",
    "        if abs(row[col_idx] - meds[col_idx]) / mads[col_idx] > 3:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "mads_broadcast = sc.broadcast(mads)\n",
    "# threshold_broadcast = sc.broadcast(threshold)\n",
    "filtered_robust_rdd = train_rdd.filter(robust_outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's print the statistical report after applying robust Z-score for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Trip duration-----\n",
      "Min: 1.0\n",
      "Max: 1595.0\n",
      "Mean: 624.868964779068\n",
      "Median: 569.0\n",
      "\n",
      "-----Direct Distance-----\n",
      "Min: 0.4236109090686456\n",
      "Max: 5277.969420283912\n",
      "Mean: 1981.7221711158975\n",
      "Median: 1723.4443160613012\n",
      "\n",
      "-----Velocity-----\n",
      "Min: 0.2577348925578789\n",
      "Max: 25.343768423001187\n",
      "Mean: 12.202033285699084\n",
      "Median: 11.678126558060292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "name_col = [\"Trip duration\",\"Direct Distance\", \"Velocity\"]\n",
    "# Because we want to collect the last 3 columns so we create \n",
    "# a broadcast variable to pass to collect_column_values function\n",
    "offset_broadcast = sc.broadcast(3)\n",
    "\n",
    "# collect value grouped by columned\n",
    "col_robust_rdd = filtered_robust_rdd.flatMap(collect_column_values)\n",
    "col_robust_grouped = col_robust_rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Calculate statistical report for each column \n",
    "robust_filtered_rdd = col_robust_grouped.mapValues(statistical_report)\n",
    "robust_report = robust_filtered_rdd.collectAsMap()\n",
    "\n",
    "# Print the statistical report for IQR filtered column\n",
    "for i in range(3):\n",
    "    print(f\"\\n-----{name_col[i]}-----\")\n",
    "    report = robust_report[num_columns_broadcast.value - 3 + i]\n",
    "    print(f\"Min: {report[0]}\")\n",
    "    print(f\"Max: {report[1]}\")\n",
    "    print(f\"Mean: {report[2]}\")\n",
    "    print(f\"Median: {report[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the statistical report, looks like it even get worse. Because the `velocity` and `direct distance` cannot be equal to zero, we can consider it as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_rdd = filtered_iqr_rdd.filter(lambda x: x[-1] > 0 and x[-1] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/10 10:43:40 WARN BlockManager: Task 70 already completed, not releasing lock for rdd_39_0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((2.0, 1.0, -74.01004028320312, 40.719970703125, -74.01226806640625, 40.70671844482422, 19, 2, 97), 429.0)\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'float'>\n",
      "889018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create an RDD where each record is a tuple: (features as a list, label)\n",
    "cleaned_data = cleaned_rdd.map(lambda cols: (cols[:-3], cols[-3])).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "first_element = cleaned_data.take(1)[0]\n",
    "print(first_element)\n",
    "print(type(first_element)) \n",
    "print(type(first_element[0]))\n",
    "print(type(first_element[1]))\n",
    "print(cleaned_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[11] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleaned_data consist of lazy evaluation operation on train_rdd, so we have to make sure that \n",
    "# train_rdd is not unpersisted before cleaned_data call a non lazy evaluation operation and being cached\n",
    "train_rdd.unpersist() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "889018"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute adaptive quantiles based on dataset size\n",
    "def get_adaptive_quantiles(rdd, feature_index, mode='sqrt'):\n",
    "    count = rdd.count()\n",
    "\n",
    "    if mode == 'sqrt':\n",
    "        num_quantiles = int(math.sqrt(count))\n",
    "    elif mode == 'log':\n",
    "        num_quantiles = int(math.log2(count))\n",
    "    else:\n",
    "        num_quantiles = 10  # fallback\n",
    "\n",
    "    values = rdd.map(lambda x: x[0][feature_index]) \\\n",
    "                .distinct() \\\n",
    "                .sortBy(lambda x: x) \\\n",
    "                .collect()\n",
    "\n",
    "    if len(values) <= num_quantiles:\n",
    "        return values\n",
    "\n",
    "    step = len(values) // (num_quantiles + 1)\n",
    "    quantiles = [values[i * step] for i in range(1, num_quantiles + 1)]\n",
    "    return quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find best split on RDD\n",
    "def find_best_split_rdd(rdd, num_features):\n",
    "    def evaluate_split(feature_index):\n",
    "        quantiles = get_adaptive_quantiles(rdd, feature_index)\n",
    "\n",
    "        best_mse = float('inf')\n",
    "        best_threshold = None\n",
    "\n",
    "        for threshold in quantiles:\n",
    "            # Return values of seq_op: sumL, sq_sumL, countL, sumR, sq_sumR, countR\n",
    "            def seq_op(acc, row):\n",
    "                value = row[0][feature_index]\n",
    "                label = row[1]\n",
    "                if value <= threshold:\n",
    "                    return (\n",
    "                        acc[0] + label,           # sumL\n",
    "                        acc[1] + label ** 2,      # sq_sumL\n",
    "                        acc[2] + 1,               # countL\n",
    "                        acc[3], acc[4], acc[5]    # right remains the same\n",
    "                    )\n",
    "                else:\n",
    "                    return (\n",
    "                        acc[0], acc[1], acc[2],   # left remains the same\n",
    "                        acc[3] + label,           # sumR\n",
    "                        acc[4] + label ** 2,      # sq_sumR\n",
    "                        acc[5] + 1                # countR\n",
    "                    )\n",
    "\n",
    "            def comb_op(acc1, acc2):\n",
    "                return tuple(a + b for a, b in zip(acc1, acc2))\n",
    "\n",
    "            # Initialized values\n",
    "            init = (0.0, 0.0, 0, 0.0, 0.0, 0)\n",
    "            sumL, sq_sumL, countL, sumR, sq_sumR, countR = rdd.aggregate(init, seq_op, comb_op)\n",
    "\n",
    "            if countL == 0 or countR == 0:\n",
    "                continue\n",
    "\n",
    "            mse_left = (sq_sumL / countL) - (sumL / countL) ** 2\n",
    "            mse_right = (sq_sumR / countR) - (sumR / countR) ** 2\n",
    "            total_mse = (countL * mse_left + countR * mse_right) / (countL + countR)\n",
    "\n",
    "            if total_mse < best_mse:\n",
    "                best_mse = total_mse\n",
    "                best_threshold = threshold\n",
    "\n",
    "        return (feature_index, best_threshold, best_mse)\n",
    "\n",
    "\n",
    "    results = [evaluate_split(i) for i in range(num_features)]\n",
    "\n",
    "    # Choose the best split\n",
    "    best_result = min(results, key=lambda x: x[2])\n",
    "    return best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/10 10:43:41 WARN BlockManager: Task 83 already completed, not releasing lock for rdd_39_0\n"
     ]
    }
   ],
   "source": [
    "# Build tree\n",
    "def build_tree_rdd(rdd, num_features, depth=0, max_depth=3, min_samples=10):\n",
    "    rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    count = rdd.count()\n",
    "    \n",
    "    # Stop condition\n",
    "    if count < min_samples or depth >= max_depth:\n",
    "        mean = rdd.map(lambda x: x[1]).reduce(lambda x, y: x + y) / count\n",
    "        return Node(value=mean)\n",
    "    \n",
    "    # Find optimal split\n",
    "    feature_index, threshold, mse = find_best_split_rdd(rdd, num_features)\n",
    "    \n",
    "    if feature_index is None or threshold is None:\n",
    "        mean = rdd.map(lambda x: x[1]).reduce(lambda x, y: x + y) / count\n",
    "        return Node(value=mean)\n",
    "    \n",
    "    # Split the RDD\n",
    "    left_rdd = rdd.filter(lambda x: x[0][feature_index] <= threshold)\n",
    "    right_rdd = rdd.filter(lambda x: x[0][feature_index] > threshold)\n",
    "    \n",
    "    # Build the branch recuresively\n",
    "    left_node = build_tree_rdd(left_rdd, num_features, depth + 1, max_depth, min_samples)\n",
    "    right_node = build_tree_rdd(right_rdd, num_features, depth + 1, max_depth, min_samples)\n",
    "    \n",
    "    rdd.unpersist()\n",
    "    \n",
    "    return Node(feature_index=feature_index, threshold=threshold, \n",
    "                left=left_node, right=right_node)\n",
    "\n",
    "\n",
    "# Build the tree\n",
    "num_features = len(cleaned_data.take(1)[0][0])\n",
    "min_samples = cleaned_data.count() // 100\n",
    "tree = build_tree_rdd(cleaned_data, num_features, 0, 3, min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/10 11:22:14 WARN BlockManager: Task 100032 already completed, not releasing lock for rdd_10_0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1.0, 1.0, -74.00899505615234, 40.712589263916016, -73.9986343383789, 40.722808837890625, 12, 5, 9), 707.0)\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'float'>\n",
      "437341\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD where each record is a tuple: (features as a list, label)\n",
    "test_data = test_rdd.map(lambda cols: (cols[:6] + cols[7:-1], cols[-1]))\n",
    "\n",
    "first_element = test_data.take(1)[0]\n",
    "print(first_element)\n",
    "print(type(first_element)) \n",
    "print(type(first_element[0]))\n",
    "print(type(first_element[1]))\n",
    "test_size =test_data.count()\n",
    "print(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[10] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_data.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm dự đoán\n",
    "def predict(tree, sample):\n",
    "    if tree.value is not None:\n",
    "        return tree.value\n",
    "    if sample[tree.feature_index] <= tree.threshold:\n",
    "        return predict(tree.left, sample)\n",
    "    return predict(tree.right, sample)\n",
    "\n",
    "prediction_rdd = test_data.map(lambda x: (x[1], predict(tree, x[0]))).persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV. Evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33351:=============================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4608.925435997215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def compute_rmse(prediction_rdd):\n",
    "    sum_squared_error, count = prediction_rdd.aggregate(\n",
    "        (0.0, 0),\n",
    "        lambda acc, x: (acc[0] + (x[0] - x[1]) ** 2, acc[1] + 1),\n",
    "        lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
    "    )\n",
    "    mse = sum_squared_error / count\n",
    "    rmse = math.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "rmse = compute_rmse(prediction_rdd)\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R²: 0.0005965947874605471\n"
     ]
    }
   ],
   "source": [
    "def r2_seq_op(acc, row):\n",
    "    y = row[0]\n",
    "    y_hat = row[1]\n",
    "    return (\n",
    "        acc[0] + y,            # sum y for mean computing\n",
    "        acc[1] + 1,            # count\n",
    "        acc[2] + (y - y_hat) ** 2,     # ss_res\n",
    "        acc[3] + y ** 2                 # ss_tot for later variance computing\n",
    "    )\n",
    "\n",
    "def r2_comb_op(a, b):\n",
    "    return tuple(x + y for x, y in zip(a, b))\n",
    "\n",
    "sum_y, count, ss_res, sum_y_sq = prediction_rdd.aggregate((0.0, 0.0, 0.0, 0.0), r2_seq_op, r2_comb_op)\n",
    "mean_y = sum_y / count\n",
    "ss_tot = sum_y_sq - (sum_y ** 2) / count\n",
    "\n",
    "r2 = 1 - ss_res / ss_tot\n",
    "print(f\"R²: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_data.unpersist\n",
    "# prediction_rdd.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### V. Predict the data from test.csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the test.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id,vendor_id,pickup_datetime,passenger_count,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,store_and_fwd_flag',\n",
       " 'id3004672,1,2016-06-30 23:59:58,1,-73.988128662109375,40.732028961181641,-73.99017333984375,40.756679534912109,N',\n",
       " 'id3505355,1,2016-06-30 23:59:53,1,-73.964202880859375,40.67999267578125,-73.959808349609375,40.655403137207031,N',\n",
       " 'id1217141,1,2016-06-30 23:59:47,1,-73.9974365234375,40.737583160400391,-73.986160278320312,40.729522705078125,N',\n",
       " 'id2150126,2,2016-06-30 23:59:41,1,-73.956069946289063,40.771900177001953,-73.986427307128906,40.73046875,N']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV file as a text file and filter out the header\n",
    "lines = sc.textFile(\"hdfs:///hcmus/22120210/Lab3/data/train.csv\")\n",
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the test.csv file to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_predict_data(line):\n",
    "    float_indices = broadcast_indices.value\n",
    "    try:\n",
    "        parts = line.strip().split(',')\n",
    "        \n",
    "        # TIME FEATURES\n",
    "        dt_start_str = parts[2]  # pick up time\n",
    "        dt_start = datetime.strptime(dt_start_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        # Extract feature: hour, weekday, day of year\n",
    "        time_features = [dt_start.hour, dt_start.weekday(), dt_start.timetuple().tm_yday]\n",
    "\n",
    "        # Parse numerical values\n",
    "        parsed = [float(parts[i]) for i in float_indices]\n",
    "        \n",
    "        return parts[0], tuple(parsed + time_features)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing line: {line} -> {e}\")\n",
    "        return None  # Print log's information and return None when encounter error in processing\n",
    "    \n",
    "predict_sample_header = lines.first().split(',')\n",
    "predict_sample_rdd = lines.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat the parse data for predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the selected features' indices in the shown above header\n",
    "float_indices = [predict_sample_header.index(col) for col in float_cols[:-1]]\n",
    "broadcast_indices = sc.broadcast(float_indices)\n",
    "\n",
    "predict_data = predict_sample_rdd.map(parse_predict_data).filter(lambda x: x is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting and processing the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_predict = predict_data.map(lambda x: (x[0], predict(tree, x[1])))\n",
    "\n",
    "# 1. Create header\n",
    "header = [(\"id\", \"trip_duration\")]\n",
    "\n",
    "# 2. Merge header with the result\n",
    "final_with_header = sc.parallelize(header).union(final_predict)\n",
    "\n",
    "# 3. Convert each row to CSV string\n",
    "csv_rdd = final_with_header.map(lambda row: \",\".join(map(str, row)))\n",
    "\n",
    "# 4. Save as text file\n",
    "csv_rdd.coalesce(1).saveAsTextFile(\"hdfs:///hcmus/22120210/Lab3/output_dir\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"hdfs:///hcmus/22120210/Lab3/output_dir\"\n",
    "final_file = \"hdfs:///hcmus/22120210/Lab3/result.csv\"\n",
    "\n",
    "# Use HDFS file system (ensure Hadoop is set up and pyarrow or hdfs library is installed)\n",
    "hdfs_client = InsecureClient('http://localhost:9870', user='khtn_22120210')\n",
    "\n",
    "# Check whether the output file has already existed or not, if yes remove the existed file\n",
    "if hdfs_client.status(final_file, strict=False):  # This checks if the file exists\n",
    "    hdfs_client.delete(final_file)\n",
    "\n",
    "# Move part-* and change its name as desired\n",
    "for f in os.listdir(output_dir):\n",
    "    if f.startswith(\"part-\"):\n",
    "        # Read the part- file, write its content into final result\n",
    "        with hdfs_client.write(final_file, overwrite=True) as writer:\n",
    "            with open(os.path.join(output_dir, f), 'rb') as reader:\n",
    "                shutil.copyfileobj(reader, writer)\n",
    "        break\n",
    "\n",
    "# Remove temporary folder on HDFS\n",
    "hdfs_client.delete(output_dir, recursive=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
