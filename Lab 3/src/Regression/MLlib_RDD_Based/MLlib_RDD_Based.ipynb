{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **MLlib RDD-Based**\n",
        "\n",
        "(last update: 9/5/2025)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_KWoGRBcgQY"
      },
      "source": [
        "# **I. Prepare enviroment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UksomsxAVvfr"
      },
      "outputs": [],
      "source": [
        "# start pyspark\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tiBYFrI_Vw1e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/10 22:02:12 WARN Utils: Your hostname, phatle1578-VMware-Virtual-Platform resolves to a loopback address: 127.0.1.1; using 192.168.23.130 instead (on interface ens33)\n",
            "25/05/10 22:02:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/05/10 22:02:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local\")\\\n",
        "          .appName(\"Spark MLlib RDD-Based Exercises\")\\\n",
        "          .config(\"spark.some.config.option\", \"some-value\")\\\n",
        "          .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jrbeCh5ncahV"
      },
      "outputs": [],
      "source": [
        "from pyspark.mllib.tree import DecisionTree\n",
        "from pyspark.mllib.regression import LabeledPoint\n",
        "\n",
        "import math\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhjgqp2y1tC4"
      },
      "source": [
        "# **II. MLlib RDD-Based Implementation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HhYGYh229vu"
      },
      "source": [
        "### **1. Read data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read raw csv data from HDFS into RDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "V0xMBV3X1vTu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "train_lines = sc.textFile(\"hdfs:///hcmus/22120262/Practical Exercises/HW3/data/train.csv\").repartition(10)\n",
        "train_header = train_lines.first()\n",
        "train_rawData = train_lines.filter(lambda line: line != train_header)\n",
        "\n",
        "test_lines = sc.textFile(\"hdfs:///hcmus/22120262/Practical Exercises/HW3/data/test.csv\").repartition(10)\n",
        "test_header = test_lines.first()\n",
        "test_data = test_lines.filter(lambda line: line != test_header)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **2. Train-val split**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We split `train_rawData` into `train_data` (for training model) and `val_data` (for validation model) with ratio 8/2.\n",
        "\n",
        "We split it right away to keep the realistic of the `val_data`. So that we can apply pre-process data on only the `train_data`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "(train_data, val_data) = train_rawData.randomSplit([0.8, 0.2], seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owWNa4dVix-P"
      },
      "source": [
        "### **3. Parsing and pre-process the data file**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We know from the `DataExplore.ipynb` that distance play a huge role in exploration our data, so I will create a function to calculate the distance between the pick up and drop off location.\n",
        "\n",
        "Beside if we use the raw coordinate data, it will be hard for the model to utilize the spatial pattern.\n",
        "\n",
        "For better implementation with Spark, I will define the haversine functoin to compute the distance manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371  # Earth radius in km\n",
        "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
        "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
        "    return R * c * 1000  # meters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply `haversine()` to calculate travel `distance_m` (as fly crow) on the datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "distances = train_data.map(lambda row: row.split(\",\")) \\\n",
        "    .filter(lambda parts: parts[5] and parts[6] and parts[7] and parts[8]) \\\n",
        "    .map(lambda parts: haversine(float(parts[6]), float(parts[5]), float(parts[8]), float(parts[7]))) \\\n",
        "    .filter(lambda d: d >= 0) \\\n",
        "    .collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we parse the dataframe into LabelPoint "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse(row, is_test=False):\n",
        "    try:\n",
        "        parts = row.split(',')\n",
        "        \n",
        "        row_id = parts[0]\n",
        "        pickup_dayofmonth = int(parts[2].split(' ')[0].split('-')[2])\n",
        "        pickup_month = int(parts[2].split(' ')[0].split('-')[1])\n",
        "        pickup_hour = int(parts[2].split(' ')[1].split(':')[0])\n",
        "        pickup_lat = 0.0\n",
        "        pickup_long = 0.0\n",
        "        dropoff_lat = 0.0\n",
        "        dropoff_long = 0.0\n",
        "        vendor_id = float(parts[1]) - 1.0\n",
        "        passenger_count = 0.0\n",
        "        store_and_fwd_flag = 0.0\n",
        "        trip_duration = 0.0\n",
        "\n",
        "        if is_test:\n",
        "            pickup_lat = float(parts[5])\n",
        "            pickup_long = float(parts[4])\n",
        "            dropoff_lat = float(parts[7])\n",
        "            dropoff_long = float(parts[6])\n",
        "            passenger_count = float(parts[3])\n",
        "            store_and_fwd_flag = (1.0 if parts[8] == 'Y' else 0.0)\n",
        "\n",
        "        else:\n",
        "            pickup_lat = float(parts[6])\n",
        "            pickup_long = float(parts[5])\n",
        "            dropoff_lat = float(parts[8])\n",
        "            dropoff_long = float(parts[7])\n",
        "            passenger_count = float(parts[4])\n",
        "            store_and_fwd_flag = (1.0 if parts[9] == 'Y' else 0.0)\n",
        "            trip_duration = float(parts[10])\n",
        "\n",
        "        distance_m = haversine(pickup_lat, pickup_long, dropoff_lat, dropoff_long)\n",
        "\n",
        "        features = [\n",
        "            vendor_id,\n",
        "            passenger_count,\n",
        "            distance_m,\n",
        "            pickup_hour,\n",
        "            pickup_dayofmonth,\n",
        "            pickup_month,\n",
        "            store_and_fwd_flag,\n",
        "        ]\n",
        "\n",
        "        # Trả về (id, features) cho test data\n",
        "        return (row_id, features) if is_test else LabeledPoint(trip_duration, features)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Lỗi tại hàng {row[:50]}... | Chi tiết: {str(e)}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the `DataExplore.ipynb` file, we know that `trip_duration` and `distance_m` is heavily skewed. Esspecially `trip_duration` which heavily right-skewed.\n",
        "\n",
        "So to filter out those outliers, I will define a function to transform them to log-scale and using MAD to determine the boundaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_outliers_mad_log_rdd(rdd, target='label', threshold=3, feature_index=None):    \n",
        "    # Extract values from LabeledPoints\n",
        "    if target == 'label':\n",
        "        values = rdd.map(lambda lp: lp.label)\n",
        "    elif target == 'feature':\n",
        "        if feature_index is None:\n",
        "            raise ValueError(\"Phải chỉ định feature_index khi target='feature'\")\n",
        "        values = rdd.map(lambda lp: lp.features[feature_index])\n",
        "    else:\n",
        "        raise ValueError(\"target phải là 'label' hoặc 'feature'\")\n",
        "    \n",
        "    # Add 1 and log transform (to avoid log(0))\n",
        "    log_values = values.map(lambda x: np.log(x + 1)).cache()\n",
        "    \n",
        "    # Compute median and MAD\n",
        "    median_log = log_values.takeOrdered(int(log_values.count() * 0.5))[-1]\n",
        "    abs_deviations = log_values.map(lambda x: abs(x - median_log)).cache()\n",
        "    mad_log = abs_deviations.takeOrdered(int(abs_deviations.count() * 0.5))[-1]\n",
        "    \n",
        "    # Compute boundaries on log scale\n",
        "    lower_bound_log = median_log - threshold * mad_log\n",
        "    upper_bound_log = median_log + threshold * mad_log\n",
        "    \n",
        "    # Chuyển về thang đo gốc\n",
        "    lower_bound = np.exp(lower_bound_log) - 1\n",
        "    upper_bound = np.exp(upper_bound_log) - 1\n",
        "    \n",
        "    # Convert back to original scale\n",
        "    if target == 'label':\n",
        "        filtered_rdd = rdd.filter(\n",
        "            lambda lp: (np.log(lp.label + 1) >= lower_bound_log) & \n",
        "                       (np.log(lp.label + 1) <= upper_bound_log)\n",
        "        )\n",
        "    else:\n",
        "        filtered_rdd = rdd.filter(\n",
        "            lambda lp: (np.log(lp.features[feature_index] + 1) >= lower_bound_log) & \n",
        "                       (np.log(lp.features[feature_index] + 1) <= upper_bound_log)\n",
        "        )\n",
        "    \n",
        "    # Clean up cached RDDs\n",
        "    log_values.unpersist()\n",
        "    abs_deviations.unpersist()\n",
        "    \n",
        "    return filtered_rdd, (lower_bound, upper_bound)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we parse our data to make it into LabelPoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_parsed = train_data.map(lambda x: parse(x, is_test=False)) \\\n",
        "                        .filter(lambda x: x is not None) \\\n",
        "                        .cache()\n",
        "\n",
        "val_parsed = val_data.map(lambda x: parse(x, is_test=False)) \\\n",
        "                    .filter(lambda x: x is not None) \\\n",
        "                    .cache()\n",
        "\n",
        "test_parsed = test_data.map(lambda x: parse(x, is_test=True)) \\\n",
        "                      .filter(lambda x: x is not None) \\\n",
        "                      .cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we filter out the outlier of `trip_duration` and `distance_m`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trip_duration limit: 148.56s - 2947.01s\n"
          ]
        }
      ],
      "source": [
        "# Lọc theo trip_duration (label)\n",
        "train_cleaned_rdd, (trip_low, trip_high) = filter_outliers_mad_log_rdd(\n",
        "    train_parsed, \n",
        "    target='label',\n",
        "    threshold=3\n",
        ")\n",
        "print(f\"trip_duration limit: {trip_low:.2f}s - {trip_high:.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "distance_m limit: 420.75s - 10748.53s\n"
          ]
        }
      ],
      "source": [
        "train_cleaned_rdd, (dist_low, dist_high) = filter_outliers_mad_log_rdd(\n",
        "    train_cleaned_rdd,\n",
        "    target='feature',\n",
        "    feature_index=2, # distance_m\n",
        "    threshold=3\n",
        ")\n",
        "print(f\"distance_m limit: {dist_low:.2f}s - {dist_high:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6dc8p6qpaAO"
      },
      "source": [
        "### **4. Train the Decision Tree Regressor model using MLlib**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define `evaluate_metrics` that use $RMSE$ and $E^2$ metrics for evaluate consistency with the Structure API implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_metrics(labelsAndPredictions):\n",
        "    metrics = labelsAndPredictions.map(\n",
        "        lambda x: (1, x[0], x[1], (x[0] - x[1]) ** 2, x[0] ** 2)\n",
        "    ).reduce(\n",
        "        lambda a, b: (\n",
        "            a[0] + b[0],                                                        # count (n)\n",
        "            a[1] + b[1],                                                        # sum of labels (sum(y))\n",
        "            a[2] + b[2],                                                        # sum of predictions (sum(y_hat))\n",
        "            a[3] + b[3],                                                        # sum of squared errors (sum((y - y_hat)^2))\n",
        "            a[4] + b[4]                                                         # sum of squared labels (sum(y^2))\n",
        "        )\n",
        "    )\n",
        "\n",
        "    n = metrics[0]\n",
        "    if n == 0:\n",
        "        return {\"RMSE\": 0, \"R2\": 0}\n",
        "\n",
        "    mse = metrics[3] / n\n",
        "    rmse = mse ** 0.5                                                           # RMSE = sqrt(MSE)\n",
        "\n",
        "    ss_total = metrics[4] - (metrics[1] ** 2) / n                               # sum((y - mean(y))^2)\n",
        "    ss_residual = metrics[3]                                                    # sum((y - y_hat)^2)\n",
        "    r2 = 1 - (ss_residual / ss_total) if ss_total != 0 else 0.0                 # R² = 1 - (SS_res / SS_total)\n",
        "\n",
        "    return {\"RMSE\": rmse, \"R2\": r2}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define Decision Tree model with parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I already hyperparameter tuning the model for optimal performance.  \n",
        "But because of the time it take for tuning, I will only show the result of the best hyperparameters and the code I use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "maxDepths = [5, 10, 15]\n",
        "maxBins_list = [32, 64]\n",
        "minInstances_list = [1, 5, 10]\n",
        "\n",
        "best_rmse = float('inf')\n",
        "best_params = {}\n",
        "\n",
        "dt_model = None\n",
        "\n",
        "for depth in maxDepths:\n",
        "    for bins in maxBins_list:\n",
        "        for min_instances in minInstances_list:\n",
        "            # Train model với các tham số hiện tại\n",
        "            current_model = DecisionTree.trainRegressor(\n",
        "                train_data,\n",
        "                categoricalFeaturesInfo={},\n",
        "                impurity=\"variance\",\n",
        "                maxDepth=depth,\n",
        "                maxBins=bins,\n",
        "                minInstancesPerNode=min_instances\n",
        "            )\n",
        "            \n",
        "            # Đánh giá model\n",
        "            predictions = current_model.predict(val_data.map(lambda x: x.features))\n",
        "            labelsAndPredictions = val_data.map(lambda lp: lp.label).zip(predictions)\n",
        "            metrics = evaluate_metrics(labelsAndPredictions)\n",
        "            \n",
        "            # Cập nhật best model nếu tốt hơn\n",
        "            if metrics[\"RMSE\"] < best_rmse:\n",
        "                best_rmse = metrics[\"RMSE\"]\n",
        "                best_params = {\n",
        "                    'maxDepth': depth,\n",
        "                    'maxBins': bins,\n",
        "                    'minInstancesPerNode': min_instances\n",
        "                }\n",
        "                print(f\"New best params: {best_params}\")\n",
        "\n",
        "                dt_model = current_model\n",
        "\n",
        "print(\"Best maxDepth:\", best_params['maxDepth'])\n",
        "print(\"Best minInstancesPerNode:\", best_params['minInstancesPerNode'])\n",
        "print(\"Best maxBins:\", best_params['maxBins'])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Best maxDepth: 10  \n",
        "> Best minInstancesPerNode: 20  \n",
        "> Best maxBins: 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "categoricalFeatures= {\n",
        "    0: 2,\n",
        "    6: 2\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NKFaFUAGpv_L"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/10 22:04:13 WARN BlockManager: Task 95 already completed, not releasing lock for rdd_17_0\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "dt_model = DecisionTree.trainRegressor(\n",
        "    train_cleaned_rdd,\n",
        "    categoricalFeaturesInfo=categoricalFeatures,\n",
        "    impurity=\"variance\",\n",
        "    maxDepth=10,\n",
        "    maxBins=64,\n",
        "    minInstancesPerNode=20\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6aAkdg9NAGa"
      },
      "source": [
        "Make predictions on validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEdRvpSXsgFE",
        "outputId": "38221f1e-e366-493f-dbe6-be4ca1ea8401"
      },
      "outputs": [],
      "source": [
        "val_predictions = dt_model.predict(val_parsed.map(lambda x: x.features))\n",
        "\n",
        "val_labelsAndPredictions = val_parsed.map(lambda lp: lp.label).zip(val_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrgAHSdGNGgD"
      },
      "source": [
        "### **5. Evaluation model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate on validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Lỗi tại hàng id,vendor_id,pickup_datetime,dropoff_datetime,pass... | Chi tiết: list index out of range\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model Evaluation Results:\n",
            "Validation Set:\n",
            "Root Mean Squared Error (RMSE) = 3146.577415965157\n",
            "R-squared (R²) = 0.025928716710519994\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "metrics = evaluate_metrics(val_labelsAndPredictions)\n",
        "\n",
        "print(\"\\nModel Evaluation Results:\")\n",
        "print(\"Validation Set:\")\n",
        "print(\"Root Mean Squared Error (RMSE) =\", metrics[\"RMSE\"])\n",
        "print(\"R-squared (R²) =\", metrics[\"R2\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **III. Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">- RMSE = 3171.3017981403855\n",
        ">\n",
        ">    - This is the standard deviation of the prediction errors.\n",
        ">\n",
        ">    - It shows that, on average, the model predictions are off by about 3171 units.\n",
        ">\n",
        ">- R² = 0.024592114716844637\n",
        ">\n",
        ">    - This means the model explains only ~2.4% of the variance in the target variable.\n",
        ">\n",
        ">    - Essentially, the model isn’t capturing much of the relationship between the features and the target.\n",
        ">\n",
        ">    - An R² this low suggests that the model is barely better than predicting the mean."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **IV. Predict on test set**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Predict on test set and save to local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Lỗi tại hàng id,vendor_id,pickup_datetime,passenger_count,picku... | Chi tiết: list index out of range\n",
            "25/05/10 22:05:10 WARN TaskSetManager: Stage 65 contains a task of very large size (18447 KiB). The maximum recommended task size is 1000 KiB.\n",
            "[Stage 65:>                                                         (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions saved successfully to local directory.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "test_predictions = dt_model.predict(test_parsed.map(lambda x: x[1]))\n",
        "\n",
        "test_idAndPredictions = test_parsed.map(lambda x: x[0]).zip(test_predictions)\n",
        "\n",
        "header = sc.parallelize([\"id,prediction\"])\n",
        "results_csv = test_idAndPredictions.map(lambda x: f\"{x[0]},{x[1]}\")\n",
        "\n",
        "output_dir = \"file:///home/phatle1578/BigData/Practical Exercises/HW3/MLlib RDD-Based/result\"\n",
        "\n",
        "full_output = sc.parallelize(\n",
        "    [\"id,prediction\"] + results_csv.collect()\n",
        ")\n",
        "\n",
        "full_output.saveAsTextFile(output_dir)\n",
        "\n",
        "print(\"Predictions saved successfully to local directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Open Terminal in the folder that store the notebook, then use this command to extract the `result.csv`.\n",
        "```\n",
        "mv result/part-00000 result.csv\n",
        "rm -r result/\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onSUhdtD5FjS"
      },
      "source": [
        "# **V. Reference**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu6KaSQI5ICj"
      },
      "source": [
        "1. [Spark Document - Decision Trees(RDD-based API)](https://spark.apache.org/docs/latest/mllib-decision-tree.html)\n",
        "\n",
        "2. [NYC Taxi EDA - Update: The fast & the curious](https://www.kaggle.com/code/headsortails/nyc-taxi-eda-update-the-fast-the-curious/report#extreme-trip-durations)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
