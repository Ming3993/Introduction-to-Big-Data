{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiHCYDNURIVX"
      },
      "source": [
        "# 1. Initialize SparkSession and load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbUFyuo7Q7pV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "your 131072x1 screen size is bogus. expect trouble\n",
            "25/05/10 13:17:47 WARN Utils: Your hostname, HuuNghia-PC resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
            "25/05/10 13:17:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/05/10 13:17:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Time: double (nullable = true)\n",
            " |-- V1: double (nullable = true)\n",
            " |-- V2: double (nullable = true)\n",
            " |-- V3: double (nullable = true)\n",
            " |-- V4: double (nullable = true)\n",
            " |-- V5: double (nullable = true)\n",
            " |-- V6: double (nullable = true)\n",
            " |-- V7: double (nullable = true)\n",
            " |-- V8: double (nullable = true)\n",
            " |-- V9: double (nullable = true)\n",
            " |-- V10: double (nullable = true)\n",
            " |-- V11: double (nullable = true)\n",
            " |-- V12: double (nullable = true)\n",
            " |-- V13: double (nullable = true)\n",
            " |-- V14: double (nullable = true)\n",
            " |-- V15: double (nullable = true)\n",
            " |-- V16: double (nullable = true)\n",
            " |-- V17: double (nullable = true)\n",
            " |-- V18: double (nullable = true)\n",
            " |-- V19: double (nullable = true)\n",
            " |-- V20: double (nullable = true)\n",
            " |-- V21: double (nullable = true)\n",
            " |-- V22: double (nullable = true)\n",
            " |-- V23: double (nullable = true)\n",
            " |-- V24: double (nullable = true)\n",
            " |-- V25: double (nullable = true)\n",
            " |-- V26: double (nullable = true)\n",
            " |-- V27: double (nullable = true)\n",
            " |-- V28: double (nullable = true)\n",
            " |-- Amount: double (nullable = true)\n",
            " |-- Class: integer (nullable = true)\n",
            "\n",
            "Total rows = 284807\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"CreditCardFraud_StructuredAPI\").getOrCreate()\n",
        "\n",
        "df = spark.read.csv(\"hdfs:///hcmus/22120227/Exercises/Lab3/data/creditcard.csv\", header=True, inferSchema=True)\n",
        "df.printSchema()\n",
        "print(f\"Total rows = {df.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Deduplicate & Missing-Value Check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1. Remove dulicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/10 13:17:54 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "[Stage 8:====>                                                    (1 + 11) / 12]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deduplicated: removed 1081 duplicate rows; now 283726 rows remain.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "n_before = df.count()\n",
        "\n",
        "df = df.dropDuplicates()  \n",
        "n_after  = df.count()\n",
        "print(f\"Deduplicated: removed {n_before - n_after} duplicate rows; now {n_after} rows remain.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2. Check for missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 14:==============>                                          (3 + 9) / 12]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+-----+\n",
            "|Time| V1| V2| V3| V4| V5| V6| V7| V8| V9|V10|V11|V12|V13|V14|V15|V16|V17|V18|V19|V20|V21|V22|V23|V24|V25|V26|V27|V28|Amount|Class|\n",
            "+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+-----+\n",
            "|   0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|     0|    0|\n",
            "+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+-----+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "missing_counts = df.select([\n",
        "    F.count(F.when(F.col(c).isNull() | F.isnan(F.col(c)), c)).alias(c)\n",
        "    for c in df.columns\n",
        "])\n",
        "missing_counts.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7t-wDBYUsWv"
      },
      "source": [
        "# 3. Feature Assembly & Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BkcnTnUER9Hn"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 26:====>                                                   (1 + 11) / 12]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
            "|features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |Class|\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
            "|[-0.529912284186556,0.873891581460326,1.34724732930113,0.145456676582257,0.414208858362661,0.10022309405219,0.711206082959649,0.1760659570625,-0.286716934699997,-0.484687683196852,0.872489590125871,0.851635859904339,-0.571745302934562,0.100974273045751,-1.51977183258512,-0.284375978261788,-0.310523584869201,-0.404247868800905,-0.823373523914155,-0.290347610865436,0.0469490671140629,0.208104855076299,-0.185548346773547,0.00103065983293288,0.0988157011025622,-0.552903603040518,-0.0732880835681738,0.0233070451077205,-1.9962718139218014,-0.328805400920012]|0    |\n",
            "|[-0.600816388115364,0.922454525911535,-0.135951820418704,-1.25991483910974,2.43982440634166,3.33020532398973,0.0871070880575762,0.949659242063477,-0.327931944022326,0.0617085198149349,-0.157387072529051,-0.233220336669169,-0.0533324475993436,0.221138816180166,1.05707087248086,0.505629784812465,-0.899687487478946,0.0818471727849734,0.357532361084866,0.348027853864187,-0.311488589011741,-0.9272331843096,-0.0452679356500729,0.979587246671944,0.0164232459149332,0.091043324055392,0.371280248055762,0.180187431297484,-1.9934917573336637,-0.34617764444462007] |0    |\n",
            "|[-1.5057791635308,-0.215325117259078,1.99129427646285,-1.63149338451985,-0.635965146821198,0.228413858079752,-0.0342656972363698,0.0426958270503592,-0.235439929340839,0.164740343425352,-1.51828765971003,-0.317422419544255,1.12053465313355,-1.52478487374754,-1.21854635680996,1.06301355860071,0.0443910911487924,-1.34080631609944,0.715632259412232,0.232752121754006,-0.101938019104751,0.148145577476984,-0.450114636372949,-0.396558396394281,0.554223963316214,-0.340492980681964,-0.335617814421999,-0.413379326245853,-1.992817804221388,-0.024691298759344258]  |0    |\n",
            "|[-0.491003017302294,0.906952627077483,1.64542281975857,-0.0835311021938869,-0.195560099221421,-0.710165000567951,0.559118538140669,0.11634025149828,-0.538190203722305,-0.122023316581271,1.33297073086808,0.24154718539358,-0.960824737330713,0.526002678860792,0.22936824295849,0.377199880619456,-0.520008382536953,0.186335971636626,0.230867162187187,0.0627566740593696,-0.16806729699576,-0.517387140513931,0.0186502119982566,0.49165164795184,-0.277795388237676,0.0438408610056926,0.253371621535344,0.111749327120591,-1.9912382266144915,-0.31726384142895053]    |0    |\n",
            "|[-0.528217504778877,0.981231846386665,1.6529880537617,-0.150715086881965,0.137047986925298,-0.203980367637953,0.583369653674636,0.16524639020934,-0.781696163028633,-0.211147388234523,1.82477182443679,1.17157436160694,0.607026344205782,0.237985709192966,0.306871732892064,0.00996234124647729,-0.332372932563785,-0.458194108838771,-0.137627144521662,0.127768328729222,-0.123569168149109,-0.272730915593521,0.0238866347332363,0.223935517009938,-0.29777697912304,0.0683001764849428,0.281018331265725,0.11155700970708,-1.9871523858713194,-0.3254108245991116]     |0    |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Scale Time & Amount\n",
        "assembler_ta = VectorAssembler(inputCols=[\"Time\", \"Amount\"], outputCol=\"ta_vec\")\n",
        "scaler_ta = StandardScaler(inputCol=\"ta_vec\", outputCol=\"ta_scaled\", withMean=True, withStd=True)\n",
        "\n",
        "# Assemble V1–V28 + ta_scaled into vector\n",
        "assembler_feats = VectorAssembler(inputCols=[f\"V{i}\" for i in range(1,29)] + [\"ta_scaled\"], outputCol=\"features\")\n",
        "\n",
        "preproc = Pipeline(stages=[assembler_ta, scaler_ta, assembler_feats])\n",
        "preproc_model = preproc.fit(df)\n",
        "df_feats = preproc_model.transform(df).select(\"features\",\"Class\")\n",
        "df_feats.show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Split into train and test sets (80/20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 37:============>                                           (3 + 10) / 13]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training count: 227364, Test count: 56362\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "train_df, test_df = df_feats.randomSplit([0.8, 0.2], seed=42)\n",
        "print(f\"Training count: {train_df.count()}, Test count: {test_df.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-0SCsEDaSul"
      },
      "source": [
        "# 5. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RDFnXNjRZb1o"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/10 13:18:08 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"Class\", maxIter=20)\n",
        "model = lr.fit(train_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm19QjetvTdm"
      },
      "source": [
        "# 6. Inspect Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intercept: -8.6098\n",
            "Top 5 Coefficients: [0.09607245925312342, -0.025869081541789372, -0.014673681899441574, 0.67517406267906, 0.0934677780903822] …\n"
          ]
        }
      ],
      "source": [
        "coeff  = model.coefficients.toArray().tolist()\n",
        "interc = model.intercept\n",
        "print(f\"Intercept: {interc:.4f}\")\n",
        "print(\"Top 5 Coefficients:\", coeff[:5], \"…\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTiRaoSQv9vE"
      },
      "source": [
        "# 7. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_c-XbWiv5sl",
        "outputId": "a174d907-5d77-44b4-fd02-140b3c074722"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROC-AUC = 0.9850\n",
            "PR-AUC  = 0.7498\n",
            "Accuracy  = 0.9992\n",
            "Precision = 0.8800\n",
            "Recall    = 0.6346\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "\n",
        "pred = model.transform(test_df)\n",
        "\n",
        "# AUC (ROC) and AUC (PR)\n",
        "e_roc = BinaryClassificationEvaluator(\n",
        "    labelCol=\"Class\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "e_pr  = BinaryClassificationEvaluator(\n",
        "    labelCol=\"Class\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderPR\")\n",
        "\n",
        "roc_auc = e_roc.evaluate(pred)\n",
        "pr_auc  = e_pr.evaluate(pred)\n",
        "\n",
        "# Accuracy, Precision, Recall\n",
        "e_acc = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Class\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "e_prec = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Class\", predictionCol=\"prediction\", metricName=\"precisionByLabel\")\n",
        "e_rec  = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Class\", predictionCol=\"prediction\", metricName=\"recallByLabel\")\n",
        "\n",
        "acc      = e_acc.evaluate(pred)\n",
        "precision= e_prec.evaluate(pred, {e_prec.metricLabel: 1.0})\n",
        "recall   = e_rec.evaluate(pred,  {e_rec.metricLabel: 1.0})\n",
        "\n",
        "print(f\"ROC-AUC = {roc_auc:.4f}\")\n",
        "print(f\"PR-AUC  = {pr_auc:.4f}\")\n",
        "print(f\"Accuracy  = {acc:.4f}\")\n",
        "print(f\"Precision = {precision:.4f}\")\n",
        "print(f\"Recall    = {recall:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. Export Test Set with All Original Columns + Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set size: 56362\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "\n",
        "# Split the original preprocessed DataFrame into train/test\n",
        "train_df, test_df = df_feats.randomSplit([0.8, 0.2], seed=42)\n",
        "print(f\"Test set size: {test_df.count()}\")\n",
        "\n",
        "# Define and train logistic regression model\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"Class\", maxIter=20)\n",
        "model = lr.fit(train_df)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions_df = model.transform(test_df)\n",
        "\n",
        "# Reattach raw features for CSV export\n",
        "raw_cols = [\"Time\"] + [f\"V{i}\" for i in range(1, 29)] + [\"Amount\", \"Class\"]\n",
        "full_df = df.select(*raw_cols)\n",
        "\n",
        "# Add row indices for join\n",
        "test_df_indexed = test_df.withColumn(\"row_id\", F.monotonically_increasing_id()).withColumnRenamed(\"Class\", \"label\")\n",
        "full_df_indexed = full_df.withColumn(\"row_id\", monotonically_increasing_id())\n",
        "\n",
        "# Join to recover original columns + prediction\n",
        "joined = test_df_indexed.join(full_df_indexed, on=\"row_id\").join(\n",
        "    predictions_df.select(\"features\", \"prediction\", \"probability\"), on=\"features\"\n",
        ")\n",
        "\n",
        "# Select required columns\n",
        "output_df = joined.select(\"Time\", *[f\"V{i}\" for i in range(1, 29)], \"Amount\", \"label\", \"prediction\")\n",
        "\n",
        "# Export to CSV\n",
        "output_df.coalesce(1).write.csv(\"results\", header=True, mode=\"overwrite\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Combine into a single file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "output_path = \"results\"\n",
        "\n",
        "# Coalesce the DataFrame to a single partition so it generates only one CSV part file\n",
        "output_df.coalesce(1).write.csv(output_path, header=True, mode=\"overwrite\")\n",
        "\n",
        "# Move the generated part file to a fixed single CSV path\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "for fname in os.listdir(output_path):\n",
        "    if fname.startswith(\"part-\") and fname.endswith(\".csv\"):\n",
        "        src_path = os.path.join(output_path, fname)\n",
        "        dest_path = \"results.csv\"\n",
        "        shutil.move(src_path, dest_path)\n",
        "        break\n",
        "\n",
        "# Remove the temporary output folder\n",
        "shutil.rmtree(output_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
