{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f73ba54a",
   "metadata": {},
   "source": [
    "# **Structured API**\n",
    "\n",
    "(last update: 7/5/2025)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe9cd9c",
   "metadata": {},
   "source": [
    "# **I. Prepare enviroment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8fb1c1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start pyspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "075abe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\")\\\n",
    "          .appName(\"Spark APIs Exercises\")\\\n",
    "          .config(\"spark.some.config.option\", \"some-value\")\\\n",
    "          .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e4c45299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769a3e84",
   "metadata": {},
   "source": [
    "# **II. Structured API Implementation (High-Level)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2194fc",
   "metadata": {},
   "source": [
    "### **1. Read data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83ad0a0",
   "metadata": {},
   "source": [
    "Read raw csv data from HDFS into Structured API Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ef68211a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_rawData = spark.read.csv(\"hdfs:///hcmus/22120262/Practical Exercises/HW3/data/train.csv\", header=True, inferSchema=True)\n",
    "test_rawData = spark.read.csv(\"hdfs:///hcmus/22120262/Practical Exercises/HW3/data/test.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a68c08",
   "metadata": {},
   "source": [
    "### **2. Train-val split**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b272458d",
   "metadata": {},
   "source": [
    "We split `train_rawData` into `train_data` (for training model) and `val_data` (for validation model) with ratio 8/2.\n",
    "\n",
    "We split it right away to keep the realistic of the `val_data`. So that we can apply pre-process data on only the `train_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "12515d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, val_data) = train_rawData.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb5c7f4",
   "metadata": {},
   "source": [
    "### **3. Pre-process data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7080a569",
   "metadata": {},
   "source": [
    "We know from the `DataExplore.ipynb` that distance play a huge role in exploration our data, so I will create a function to calculate the distance between the pick up and drop off location.\n",
    "\n",
    "Beside if we use the raw coordinate data, it will be hard for the model to utilize the spatial pattern.\n",
    "\n",
    "For better implementation with Spark, I will define the haversine functoin to compute the distance manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "39ce3d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    R = 6371  # radius of Earth in km\n",
    "    lon1, lat1, lon2, lat2 = map(math.radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = math.sin(dlat/2)**2 + math.cos(lat1)*math.cos(lat2)*math.sin(dlon/2)**2\n",
    "    \n",
    "    return R * 2 * math.asin(math.sqrt(a)) * 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124d2e7a",
   "metadata": {},
   "source": [
    "Apply `haversine()` to calculate travel `distance_m` (as fly crow) on the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "21e39acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "haversine_udf = F.udf(haversine, DoubleType())\n",
    "\n",
    "for df_name, df in zip(['train_data', 'test_rawData', 'val_data'], [train_data, test_rawData, val_data]):\n",
    "    df = df.withColumn(\"distance_m\", haversine_udf(\n",
    "        df[\"pickup_longitude\"], df[\"pickup_latitude\"],\n",
    "        df[\"dropoff_longitude\"], df[\"dropoff_latitude\"]\n",
    "    ))\n",
    "    globals()[df_name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac24d26",
   "metadata": {},
   "source": [
    "Based on the `DataExplore.ipynb` file, we know that `trip_duration` is heavily right-skewed on log-scale plot.  \n",
    "\n",
    "So to filter out those outliers, I will define a function to transform them to log-scale and using MAD to determine the boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e997dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_outliers_mad_log(df, column_name, threshold=3):\n",
    "    # Add log-transformed column (+1 to avoid log(0))\n",
    "    df = df.withColumn(f\"log_{column_name}\", F.log(F.col(column_name) + 1))\n",
    "    \n",
    "    # Compute Median and MAD on log-scale\n",
    "    median_log = df.approxQuantile(f\"log_{column_name}\", [0.5], 0.01)[0]\n",
    "    mad_log = df.select(\n",
    "        F.expr(f\"percentile_approx(abs(log_{column_name} - {median_log}), 0.5)\")\n",
    "    ).first()[0]\n",
    "    \n",
    "    # Compute boundaries\n",
    "    lower_bound_log = median_log - threshold * mad_log\n",
    "    upper_bound_log = median_log + threshold * mad_log\n",
    "    \n",
    "    # Convert back to original scale\n",
    "    lower_bound = np.exp(lower_bound_log) - 1\n",
    "    upper_bound = np.exp(upper_bound_log) - 1\n",
    "    \n",
    "    # Filter data\n",
    "    df_filtered = df.filter(\n",
    "        (F.col(f\"log_{column_name}\") >= lower_bound_log) & \n",
    "        (F.col(f\"log_{column_name}\") <= upper_bound_log)\n",
    "    ).drop(f\"log_{column_name}\")\n",
    "    \n",
    "    return df_filtered, (lower_bound, upper_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825a6337",
   "metadata": {},
   "source": [
    "Filter outlier for trip_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3416daca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 100:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip_duration limit: 150.51s - 2979.62s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_cleaned, (trip_low, trip_high) = filter_outliers_mad_log(\n",
    "    train_data, \n",
    "    \"trip_duration\", \n",
    "    threshold=3\n",
    ")\n",
    "print(f\"trip_duration limit: {trip_low:.2f}s - {trip_high:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9072d91",
   "metadata": {},
   "source": [
    "Now we will filter out the outlier in the `distance_m` column using the same function I define above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "42c050b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 104:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance_m limit: 422.17m - 10750.44m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_cleaned, (dist_low, dist_high) = filter_outliers_mad_log(\n",
    "    train_cleaned, \n",
    "    \"distance_m\", \n",
    "    threshold=3\n",
    ")\n",
    "print(f\"distance_m limit: {dist_low:.2f}m - {dist_high:.2f}m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228807fd",
   "metadata": {},
   "source": [
    "Handle `pickup_datetime` column which have timestamp data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b952c7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in zip(['train_cleaned', 'test_rawData', 'val_data'], [train_cleaned, test_rawData, val_data]):\n",
    "    df = df.withColumn(\"pickup_hour\", F.hour(df[\"pickup_datetime\"]))\n",
    "    df = df.withColumn(\"pickup_dayofmonth\", F.dayofmonth(df[\"pickup_datetime\"]))\n",
    "    df = df.withColumn(\"pickup_month\", F.month(df[\"pickup_datetime\"]))\n",
    "    globals()[df_name] = df\n",
    "\n",
    "train_cleaned = train_cleaned.drop(\"pickup_datetime\", \"dropoff_datetime\")\n",
    "test_data = test_rawData.drop(\"pickup_datetime\")\n",
    "val_data = val_data.drop(\"pickup_datetime\", \"dropoff_datetime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd41897",
   "metadata": {},
   "source": [
    "Handle `store_and_fwd_flag` column which have string data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a5344c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol=\"store_and_fwd_flag\", outputCol=\"store_and_fwd_flag_index\")\n",
    "indexer_model = indexer.fit(train_cleaned)\n",
    "\n",
    "train_cleaned = indexer_model.transform(train_cleaned).drop(\"store_and_fwd_flag\")\n",
    "test_data = indexer_model.transform(test_rawData).drop(\"store_and_fwd_flag\")\n",
    "val_data = indexer_model.transform(val_data).drop(\"store_and_fwd_flag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478885dc",
   "metadata": {},
   "source": [
    "Define input column (exclude lat/long raw $\\Rightarrow$ we use `distance_m` instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b66880c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_cols = [\"trip_duration\", \"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\", \"id\"]\n",
    "inputCols = [col for col in train_cleaned.columns if col not in exclude_cols]\n",
    "val_data = val_data.select(inputCols + [\"trip_duration\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346df44a",
   "metadata": {},
   "source": [
    "Assemble numeric feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5d7d84fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=inputCols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b7c3af",
   "metadata": {},
   "source": [
    "Feature indexing - handle categorical features automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0905820d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "feature_indexer = VectorIndexer(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"indexedFeatures\",\n",
    "    maxCategories=4 \n",
    ").fit(assembler.transform(train_cleaned))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88531747",
   "metadata": {},
   "source": [
    "### **4. Train the Decision Tree Regressor model using MLlib**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b84a3c7",
   "metadata": {},
   "source": [
    "Define Decision Tree model with parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7417408",
   "metadata": {},
   "source": [
    "I already run `CrossValidator` with `ParamGridBuilder` to hyperparameter tuning the model. Because of the time it take for tuning, I will only show the result of the best hyperparameters and the code I use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0b71ef",
   "metadata": {},
   "source": [
    "```\n",
    "dt = DecisionTreeRegressor(\n",
    "    featuresCol=\"indexedFeatures\",\n",
    "    labelCol=\"trip_duration\",           # Prevent overfitting\n",
    "    impurity=\"variance\"                 # Variance for regression\n",
    ")\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(dt.minInstancesPerNode, [5, 10, 20]) \\\n",
    "    .addGrid(dt.maxBins, [32, 64]) \\a\n",
    "    .build()\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=rmse_evaluator,\n",
    "    numFolds=3,\n",
    "    parallelism=2\n",
    ")\n",
    "\n",
    "cvModel = cv.fit(train_cleaned)\n",
    "\n",
    "best_dt_model = cvModel.bestModel.stages[-1]\n",
    "\n",
    "print(\"Best maxDepth:\", best_dt_model.getOrDefault('maxDepth'))\n",
    "print(\"Best minInstancesPerNode:\", best_dt_model.getOrDefault('minInstancesPerNode'))\n",
    "print(\"Best maxBins:\", best_dt_model.getOrDefault('maxBins'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c227fa9",
   "metadata": {},
   "source": [
    "> Best maxDepth: 10  \n",
    "> Best minInstancesPerNode: 20  \n",
    "> Best maxBins: 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f6a51706",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(\n",
    "    featuresCol=\"indexedFeatures\",\n",
    "    labelCol=\"trip_duration\",\n",
    "    maxBins=64,\n",
    "    maxDepth=10,                        \n",
    "    minInstancesPerNode=20,             \n",
    "    impurity=\"variance\"                 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6068ce85",
   "metadata": {},
   "source": [
    "Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "77ce0058",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    assembler,\n",
    "    feature_indexer,\n",
    "    dt\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a173a0e3",
   "metadata": {},
   "source": [
    "Create evaluators for $RMSE$ metrics and $R^2$ metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ee5a8c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"trip_duration\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "r2_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"trip_duration\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7e59dc",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2fbec9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(train_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c928dc",
   "metadata": {},
   "source": [
    "Make predictions on validation data set (`val_data`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5ddfea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = model.transform(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cefd23",
   "metadata": {},
   "source": [
    "### **5. Evaluation model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec55a38",
   "metadata": {},
   "source": [
    "Analyze model structure and feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "31ecbb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree Model Summary:\n",
      "Depth: 10\n",
      "Number of Nodes: 1997\n",
      "Feature importances:\n",
      "- vendor_id: 0.00\n",
      "- passenger_count: 0.00\n",
      "- distance_m: 0.88\n",
      "- pickup_hour: 0.10\n",
      "- pickup_dayofmonth: 0.01\n",
      "- pickup_month: 0.01\n",
      "- store_and_fwd_flag_index: 0.00\n"
     ]
    }
   ],
   "source": [
    "tree_model = model.stages[2]                                        # DecisionTreeRegressor is the 3rd stage in pipeline\n",
    "\n",
    "print(\"\\nDecision Tree Model Summary:\")\n",
    "print(\"Depth:\", tree_model.depth)\n",
    "print(\"Number of Nodes:\", tree_model.numNodes)\n",
    "print(\"Feature importances:\")\n",
    "for col, imp in zip(inputCols, tree_model.featureImportances):\n",
    "    print(f\"- {col}: {imp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d689df",
   "metadata": {},
   "source": [
    "Evaluate on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6877e1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 136:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Evaluation Results:\n",
      "Validation Set:\n",
      "Root Mean Squared Error (RMSE) = 4921.696555070559\n",
      "R-squared (R²) = 0.010458172159521717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "val_rmse = rmse_evaluator.evaluate(val_predictions)\n",
    "val_r2 = r2_evaluator.evaluate(val_predictions)\n",
    "\n",
    "print(\"\\nModel Evaluation Results:\")\n",
    "print(\"Validation Set:\")\n",
    "print(\"Root Mean Squared Error (RMSE) =\", val_rmse)\n",
    "print(\"R-squared (R²) =\", val_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84af7747",
   "metadata": {},
   "source": [
    "# **III. Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a6cc6e",
   "metadata": {},
   "source": [
    ">- RMSE = 4922.523202284945\n",
    ">\n",
    ">    - This is the standard deviation of the prediction errors.\n",
    ">\n",
    ">    - It shows that, on average, the model predictions are off by about 4922 units.\n",
    ">\n",
    ">- R² = 0.010125737730833029\n",
    ">\n",
    ">    - This means the model explains only ~1% of the variance in the target variable.\n",
    ">\n",
    ">    - Essentially, the model isn’t capturing much of the relationship between the features and the target.\n",
    ">\n",
    ">    - An R² this low suggests that the model is barely better than predicting the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689b8e8d",
   "metadata": {},
   "source": [
    "# **IV. Reference**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c737213",
   "metadata": {},
   "source": [
    "1. [Spark Document - Decistion Tree Regression](https://spark.apache.org/docs/latest/ml-classification-regression.html#decision-tree-regression)\n",
    "\n",
    "2. [NYC Taxi EDA - Update: The fast & the curious](https://www.kaggle.com/code/headsortails/nyc-taxi-eda-update-the-fast-the-curious/report#extreme-trip-durations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
