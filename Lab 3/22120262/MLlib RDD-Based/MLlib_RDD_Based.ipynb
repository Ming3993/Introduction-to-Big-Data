{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **MLlib RDD-Based**\n",
        "\n",
        "(last update: 9/5/2025)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_KWoGRBcgQY"
      },
      "source": [
        "# **I. Prepare enviroment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "UksomsxAVvfr"
      },
      "outputs": [],
      "source": [
        "# start pyspark\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "tiBYFrI_Vw1e"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local\")\\\n",
        "          .appName(\"Spark MLlib RDD-Based Exercises\")\\\n",
        "          .config(\"spark.some.config.option\", \"some-value\")\\\n",
        "          .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "jrbeCh5ncahV"
      },
      "outputs": [],
      "source": [
        "from pyspark.mllib.tree import DecisionTree\n",
        "from pyspark.mllib.regression import LabeledPoint\n",
        "\n",
        "import math\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhjgqp2y1tC4"
      },
      "source": [
        "# **II. MLlib RDD-Based Implementation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HhYGYh229vu"
      },
      "source": [
        "### **1. Read data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read raw csv data from HDFS into RDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "V0xMBV3X1vTu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/10 17:42:52 WARN BlockReaderFactory: I/O error constructing remote block reader.\n",
            "java.nio.channels.ClosedByInterruptException\n",
            "\tat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)\n",
            "\tat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:658)\n",
            "\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:191)\n",
            "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)\n",
            "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3033)\n",
            "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
            "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
            "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
            "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:755)\n",
            "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:685)\n",
            "\tat org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1647)\n",
            "\tat org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:851)\n",
            "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:893)\n",
            "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)\n",
            "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
            "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
            "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
            "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
            "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
            "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:261)\n",
            "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:50)\n",
            "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:317)\n",
            "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:247)\n",
            "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
            "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
            "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
            "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
            "25/05/10 17:42:52 WARN BlockReaderFactory: I/O error constructing remote block reader.\n",
            "java.nio.channels.ClosedByInterruptException\n",
            "\tat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)\n",
            "\tat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:658)\n",
            "\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:191)\n",
            "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)\n",
            "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3033)\n",
            "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)\n",
            "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)\n",
            "\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)\n",
            "\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:755)\n",
            "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:685)\n",
            "\tat org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1647)\n",
            "\tat org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:851)\n",
            "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:893)\n",
            "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)\n",
            "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
            "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
            "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n",
            "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n",
            "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
            "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:261)\n",
            "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:50)\n",
            "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:317)\n",
            "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:247)\n",
            "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
            "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
            "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
            "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
            "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
            "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
            "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
            "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n"
          ]
        }
      ],
      "source": [
        "train_lines = sc.textFile(\"hdfs:///hcmus/22120262/Practical Exercises/HW3/data/train.csv\")\n",
        "train_header = train_lines.first()\n",
        "train_rawData = train_lines.filter(lambda line: line != train_header)\n",
        "\n",
        "test_lines = sc.textFile(\"hdfs:///hcmus/22120262/Practical Exercises/HW3/data/test.csv\")\n",
        "test_header = test_lines.first()\n",
        "test_data = test_lines.filter(lambda line: line != test_header)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **2. Train-val split**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We split `train_rawData` into `train_data` (for training model) and `val_data` (for validation model) with ratio 8/2.\n",
        "\n",
        "We split it right away to keep the realistic of the `val_data`. So that we can apply pre-process data on only the `train_data`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "(train_data, val_data) = train_rawData.randomSplit([0.8, 0.2], seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owWNa4dVix-P"
      },
      "source": [
        "### **3. Parsing and pre-process the data file**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We know from the `DataExplore.ipynb` that distance play a huge role in exploration our data, so I will create a function to calculate the distance between the pick up and drop off location.\n",
        "\n",
        "Beside if we use the raw coordinate data, it will be hard for the model to utilize the spatial pattern.\n",
        "\n",
        "For better implementation with Spark, I will define the haversine functoin to compute the distance manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371  # Earth radius in km\n",
        "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
        "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
        "    return R * c * 1000  # meters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply `haversine()` to calculate travel `distance_m` (as fly crow) on the datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "distances = train_data.map(lambda row: row.split(\",\")) \\\n",
        "    .filter(lambda parts: parts[5] and parts[6] and parts[7] and parts[8]) \\\n",
        "    .map(lambda parts: haversine(float(parts[6]), float(parts[5]), float(parts[8]), float(parts[7]))) \\\n",
        "    .filter(lambda d: d >= 0) \\\n",
        "    .collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we parse the dataframe into LabelPoint "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse(row, is_test=False):\n",
        "    try:\n",
        "        parts = row.split(',')\n",
        "        \n",
        "        row_id = parts[0]\n",
        "        pickup_dayofmonth = int(parts[2].split(' ')[0].split('-')[2])\n",
        "        pickup_month = int(parts[2].split(' ')[0].split('-')[1])\n",
        "        pickup_hour = int(parts[2].split(' ')[1].split(':')[0])\n",
        "        pickup_lat = 0.0\n",
        "        pickup_long = 0.0\n",
        "        dropoff_lat = 0.0\n",
        "        dropoff_long = 0.0\n",
        "        vendor_id = float(parts[1]) - 1.0\n",
        "        passenger_count = 0.0\n",
        "        store_and_fwd_flag = 0.0\n",
        "        trip_duration = 0.0\n",
        "\n",
        "        if is_test:\n",
        "            pickup_lat = float(parts[5])\n",
        "            pickup_long = float(parts[4])\n",
        "            dropoff_lat = float(parts[7])\n",
        "            dropoff_long = float(parts[6])\n",
        "            passenger_count = float(parts[3])\n",
        "            store_and_fwd_flag = (1.0 if parts[8] == 'Y' else 0.0)\n",
        "\n",
        "        else:\n",
        "            pickup_lat = float(parts[6])\n",
        "            pickup_long = float(parts[5])\n",
        "            dropoff_lat = float(parts[8])\n",
        "            dropoff_long = float(parts[7])\n",
        "            passenger_count = float(parts[4])\n",
        "            store_and_fwd_flag = (1.0 if parts[9] == 'Y' else 0.0)\n",
        "            trip_duration = float(parts[10])\n",
        "\n",
        "        distance_m = haversine(pickup_lat, pickup_long, dropoff_lat, dropoff_long)\n",
        "\n",
        "        features = [\n",
        "            vendor_id,\n",
        "            passenger_count,\n",
        "            distance_m,\n",
        "            pickup_hour,\n",
        "            pickup_dayofmonth,\n",
        "            pickup_month,\n",
        "            store_and_fwd_flag,\n",
        "        ]\n",
        "\n",
        "        # Trả về (id, features) cho test data\n",
        "        return (row_id, features) if is_test else LabeledPoint(trip_duration, features)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Lỗi tại hàng {row[:50]}... | Chi tiết: {str(e)}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the `DataExplore.ipynb` file, we know that `trip_duration` and `distance_m` is heavily skewed. Esspecially `trip_duration` which heavily right-skewed.\n",
        "\n",
        "So to filter out those outliers, I will define a function to transform them to log-scale and using MAD to determine the boundaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_outliers_mad_log_rdd(rdd, target='label', threshold=3, feature_index=None):    \n",
        "    # Extract values from LabeledPoints\n",
        "    if target == 'label':\n",
        "        values = rdd.map(lambda lp: lp.label)\n",
        "    elif target == 'feature':\n",
        "        if feature_index is None:\n",
        "            raise ValueError(\"Phải chỉ định feature_index khi target='feature'\")\n",
        "        values = rdd.map(lambda lp: lp.features[feature_index])\n",
        "    else:\n",
        "        raise ValueError(\"target phải là 'label' hoặc 'feature'\")\n",
        "    \n",
        "    # Add 1 and log transform (to avoid log(0))\n",
        "    log_values = values.map(lambda x: np.log(x + 1)).cache()\n",
        "    \n",
        "    # Compute median and MAD\n",
        "    median_log = log_values.takeOrdered(int(log_values.count() * 0.5))[-1]\n",
        "    abs_deviations = log_values.map(lambda x: abs(x - median_log)).cache()\n",
        "    mad_log = abs_deviations.takeOrdered(int(abs_deviations.count() * 0.5))[-1]\n",
        "    \n",
        "    # Compute boundaries on log scale\n",
        "    lower_bound_log = median_log - threshold * mad_log\n",
        "    upper_bound_log = median_log + threshold * mad_log\n",
        "    \n",
        "    # Chuyển về thang đo gốc\n",
        "    lower_bound = np.exp(lower_bound_log) - 1\n",
        "    upper_bound = np.exp(upper_bound_log) - 1\n",
        "    \n",
        "    # Convert back to original scale\n",
        "    if target == 'label':\n",
        "        filtered_rdd = rdd.filter(\n",
        "            lambda lp: (np.log(lp.label + 1) >= lower_bound_log) & \n",
        "                       (np.log(lp.label + 1) <= upper_bound_log)\n",
        "        )\n",
        "    else:\n",
        "        filtered_rdd = rdd.filter(\n",
        "            lambda lp: (np.log(lp.features[feature_index] + 1) >= lower_bound_log) & \n",
        "                       (np.log(lp.features[feature_index] + 1) <= upper_bound_log)\n",
        "        )\n",
        "    \n",
        "    # Clean up cached RDDs\n",
        "    log_values.unpersist()\n",
        "    abs_deviations.unpersist()\n",
        "    \n",
        "    return filtered_rdd, (lower_bound, upper_bound)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we parse our data to make it into LabelPoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_parsed = train_data.map(lambda x: parse(x, is_test=False)) \\\n",
        "                        .filter(lambda x: x is not None) \\\n",
        "                        .cache()\n",
        "\n",
        "val_parsed = val_data.map(lambda x: parse(x, is_test=False)) \\\n",
        "                    .filter(lambda x: x is not None) \\\n",
        "                    .cache()\n",
        "\n",
        "test_parsed = test_data.map(lambda x: parse(x, is_test=True)) \\\n",
        "                      .filter(lambda x: x is not None) \\\n",
        "                      .cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we filter out the outlier of `trip_duration` and `distance_m`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trip_duration limit: 148.20s - 2945.13s\n"
          ]
        }
      ],
      "source": [
        "# Lọc theo trip_duration (label)\n",
        "train_cleaned_rdd, (trip_low, trip_high) = filter_outliers_mad_log_rdd(\n",
        "    train_parsed, \n",
        "    target='label',\n",
        "    threshold=3\n",
        ")\n",
        "print(f\"trip_duration limit: {trip_low:.2f}s - {trip_high:.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "distance_m limit: 421.64s - 10716.25s\n"
          ]
        }
      ],
      "source": [
        "train_cleaned_rdd, (dist_low, dist_high) = filter_outliers_mad_log_rdd(\n",
        "    train_cleaned_rdd,\n",
        "    target='feature',\n",
        "    feature_index=2, # distance_m\n",
        "    threshold=3\n",
        ")\n",
        "print(f\"distance_m limit: {dist_low:.2f}s - {dist_high:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6dc8p6qpaAO"
      },
      "source": [
        "### **4. Train the Decision Tree Regressor model using MLlib**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define `evaluate_metrics` that use $RMSE$ and $E^2$ metrics for evaluate consistency with the Structure API implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_metrics(labelsAndPredictions):\n",
        "    metrics = labelsAndPredictions.map(\n",
        "        lambda x: (1, x[0], x[1], (x[0] - x[1]) ** 2, x[0] ** 2)\n",
        "    ).reduce(\n",
        "        lambda a, b: (\n",
        "            a[0] + b[0],                                                        # count (n)\n",
        "            a[1] + b[1],                                                        # sum of labels (sum(y))\n",
        "            a[2] + b[2],                                                        # sum of predictions (sum(y_hat))\n",
        "            a[3] + b[3],                                                        # sum of squared errors (sum((y - y_hat)^2))\n",
        "            a[4] + b[4]                                                         # sum of squared labels (sum(y^2))\n",
        "        )\n",
        "    )\n",
        "\n",
        "    n = metrics[0]\n",
        "    if n == 0:\n",
        "        return {\"RMSE\": 0, \"R2\": 0}\n",
        "\n",
        "    mse = metrics[3] / n\n",
        "    rmse = mse ** 0.5                                                           # RMSE = sqrt(MSE)\n",
        "\n",
        "    ss_total = metrics[4] - (metrics[1] ** 2) / n                               # sum((y - mean(y))^2)\n",
        "    ss_residual = metrics[3]                                                    # sum((y - y_hat)^2)\n",
        "    r2 = 1 - (ss_residual / ss_total) if ss_total != 0 else 0.0                 # R² = 1 - (SS_res / SS_total)\n",
        "\n",
        "    return {\"RMSE\": rmse, \"R2\": r2}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define Decision Tree model with parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I already hyperparameter tuning the model for optimal performance.  \n",
        "But because of the time it take for tuning, I will only show the result of the best hyperparameters and the code I use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "maxDepths = [5, 10, 15]\n",
        "maxBins_list = [32, 64]\n",
        "minInstances_list = [1, 5, 10]\n",
        "\n",
        "best_rmse = float('inf')\n",
        "best_params = {}\n",
        "\n",
        "dt_model = None\n",
        "\n",
        "for depth in maxDepths:\n",
        "    for bins in maxBins_list:\n",
        "        for min_instances in minInstances_list:\n",
        "            # Train model với các tham số hiện tại\n",
        "            current_model = DecisionTree.trainRegressor(\n",
        "                train_data,\n",
        "                categoricalFeaturesInfo={},\n",
        "                impurity=\"variance\",\n",
        "                maxDepth=depth,\n",
        "                maxBins=bins,\n",
        "                minInstancesPerNode=min_instances\n",
        "            )\n",
        "            \n",
        "            # Đánh giá model\n",
        "            predictions = current_model.predict(val_data.map(lambda x: x.features))\n",
        "            labelsAndPredictions = val_data.map(lambda lp: lp.label).zip(predictions)\n",
        "            metrics = evaluate_metrics(labelsAndPredictions)\n",
        "            \n",
        "            # Cập nhật best model nếu tốt hơn\n",
        "            if metrics[\"RMSE\"] < best_rmse:\n",
        "                best_rmse = metrics[\"RMSE\"]\n",
        "                best_params = {\n",
        "                    'maxDepth': depth,\n",
        "                    'maxBins': bins,\n",
        "                    'minInstancesPerNode': min_instances\n",
        "                }\n",
        "                print(f\"New best params: {best_params}\")\n",
        "\n",
        "                dt_model = current_model\n",
        "\n",
        "print(\"Best maxDepth:\", best_params['maxDepth'])\n",
        "print(\"Best minInstancesPerNode:\", best_params['minInstancesPerNode'])\n",
        "print(\"Best maxBins:\", best_params['maxBins'])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Best maxDepth: 10  \n",
        "> Best minInstancesPerNode: 20  \n",
        "> Best maxBins: 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "categoricalFeatures= {\n",
        "    0: 2,\n",
        "    6: 2\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "NKFaFUAGpv_L"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/10 17:44:13 WARN BlockManager: Task 20 already completed, not releasing lock for rdd_7_0\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "dt_model = DecisionTree.trainRegressor(\n",
        "    train_cleaned_rdd,\n",
        "    categoricalFeaturesInfo=categoricalFeatures,\n",
        "    impurity=\"variance\",\n",
        "    maxDepth=10,\n",
        "    maxBins=64,\n",
        "    minInstancesPerNode=20\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6aAkdg9NAGa"
      },
      "source": [
        "Make predictions on validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEdRvpSXsgFE",
        "outputId": "38221f1e-e366-493f-dbe6-be4ca1ea8401"
      },
      "outputs": [],
      "source": [
        "val_predictions = dt_model.predict(val_parsed.map(lambda x: x.features))\n",
        "\n",
        "val_labelsAndPredictions = val_parsed.map(lambda lp: lp.label).zip(val_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrgAHSdGNGgD"
      },
      "source": [
        "### **5. Evaluation model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate on validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 36:=============================>                            (1 + 1) / 2]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model Evaluation Results:\n",
            "Validation Set:\n",
            "Root Mean Squared Error (RMSE) = 3171.4570734151303\n",
            "R-squared (R²) = 0.02449659530880599\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "metrics = evaluate_metrics(val_labelsAndPredictions)\n",
        "\n",
        "print(\"\\nModel Evaluation Results:\")\n",
        "print(\"Validation Set:\")\n",
        "print(\"Root Mean Squared Error (RMSE) =\", metrics[\"RMSE\"])\n",
        "print(\"R-squared (R²) =\", metrics[\"R2\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **III. Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">- RMSE = 3171.3017981403855\n",
        ">\n",
        ">    - This is the standard deviation of the prediction errors.\n",
        ">\n",
        ">    - It shows that, on average, the model predictions are off by about 3171 units.\n",
        ">\n",
        ">- R² = 0.024592114716844637\n",
        ">\n",
        ">    - This means the model explains only ~2.4% of the variance in the target variable.\n",
        ">\n",
        ">    - Essentially, the model isn’t capturing much of the relationship between the features and the target.\n",
        ">\n",
        ">    - An R² this low suggests that the model is barely better than predicting the mean."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **IV. Predict on test set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/10 17:46:35 WARN TaskSetManager: Stage 39 contains a task of very large size (18467 KiB). The maximum recommended task size is 1000 KiB.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions saved successfully to local directory.\n"
          ]
        }
      ],
      "source": [
        "test_predictions = dt_model.predict(test_parsed.map(lambda x: x[1]))\n",
        "\n",
        "test_idAndPredictions = test_parsed.map(lambda x: x[0]).zip(test_predictions)\n",
        "\n",
        "# Tạo RDD định dạng CSV\n",
        "header = sc.parallelize([\"id,prediction\"])\n",
        "results_csv = test_idAndPredictions.map(lambda x: f\"{x[0]},{x[1]}\")\n",
        "\n",
        "# Ghi ra thư mục local\n",
        "output_dir = \"file:///home/phatle1578/BigData/Practical Exercises/HW3/MLlib RDD-Based/result/submission\"\n",
        "\n",
        "full_output = sc.parallelize(\n",
        "    [\"id,prediction\"] + results_csv.collect()\n",
        ")\n",
        "\n",
        "full_output.saveAsTextFile(output_dir)\n",
        "\n",
        "print(\"Predictions saved successfully to local directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onSUhdtD5FjS"
      },
      "source": [
        "# **V. Reference**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu6KaSQI5ICj"
      },
      "source": [
        "1. [Spark Document - Decision Trees(RDD-based API)](https://spark.apache.org/docs/latest/mllib-decision-tree.html)\n",
        "\n",
        "2. [NYC Taxi EDA - Update: The fast & the curious](https://www.kaggle.com/code/headsortails/nyc-taxi-eda-update-the-fast-the-curious/report#extreme-trip-durations)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
